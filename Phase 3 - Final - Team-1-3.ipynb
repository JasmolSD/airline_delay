{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75593088-52af-45e0-b888-c01b1ef6251f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import StringIndexerModel, OneHotEncoderModel, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder,  VectorAssembler, MinMaxScaler, Imputer, PCA\n",
    "from pyspark.sql.functions import col, count, mean, stddev, min, max, corr, isnan, when, percentile_approx, regexp_replace, sum, to_date, unix_timestamp, date_format, rand, year\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "from datetime import timedelta\n",
    "import builtins\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1eed0d3-47ea-4833-a264-e2252aea5d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345d997f-e2f2-4768-843b-0ef17796d148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60_months = spark.read.parquet(\"dbfs:/mnt/mids-w261/OTPW_60M_Backup/\")\n",
    "display(df_otpw_60_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2074e51-44af-4e8d-b978-388f7932d738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbc2b06-9a65-49cc-8256-70d48803346f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Redefining clean dataset to create target variable and derived features\n",
    "\n",
    "- added derived feature for rolling past 3 month number of delays per airline\n",
    "- changed is holiday to be +- 3 days\n",
    "- added back in the delay target since this doesnt exist in the 60 m data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9775e985-878a-459f-aad0-0cc8f689abcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    # First, derive features (including DELAY_TARGET)\n",
    "    df = derive_features(df)\n",
    "    # Now, remove duplicate rows based on all columns including DELAY_TARGET\n",
    "    df = df.dropDuplicates()\n",
    "    # Continue with further transformations\n",
    "    df = transform_features(df)\n",
    "    df = create_time_based_feature(df)\n",
    "    df = subset_columns(df)\n",
    "\n",
    "    df = df.orderBy(\"FL_DATE\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def derive_features(df):\n",
    "\n",
    "    #create DELAY_TARGET as target variable\n",
    "    # Check if 'ARR_DELAY' exists in the DataFrame\n",
    "    if \"ARR_DELAY\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"DELAY_TARGET\",\n",
    "            when((col(\"CANCELLED\") == 1) | (col(\"DIVERTED\") == 1), 1)\n",
    "            .when(col(\"ARR_DELAY\") < 15, 0)\n",
    "            .otherwise(1)\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: Assume flight is delayed if cancelled/diverted otherwise keep existing values\n",
    "        df = df.withColumn(\n",
    "            \"DELAY_TARGET\",\n",
    "            when((col(\"CANCELLED\") == 1) | (col(\"DIVERTED\") == 1), 1)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "\n",
    "    #create HAS_WEATHER_TYPE if Hourly Present Weather Type is not null\n",
    "    df = df.withColumn(\n",
    "        \"HAS_WEATHER_TYPE\",\n",
    "        when(col(\"HourlyPresentWeatherType\").isNull(), 0)\n",
    "        .otherwise(1)\n",
    "    )\n",
    "\n",
    "    # create IS_HOLIDAY\n",
    "    # Step 1: Convert FL_DATE to Spark DateType\n",
    "    df = df.withColumn(\"FL_DATE\", to_date(col(\"FL_DATE\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Step 2: Generate US federal holidays for 2015 via Pandas\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holidays = cal.holidays(start=\"2015-01-01\", end=\"2019-12-31\")\n",
    "    \n",
    "    # Step 3: Expand each holiday to Â±3 days and convert to string\n",
    "    expanded_holiday_dates = set()\n",
    "    for holiday in holidays:\n",
    "        for offset in range(-3, 4):  # -3, -2, -1, 0, 1, 2, 3\n",
    "            expanded_date = holiday + timedelta(days=offset)\n",
    "            expanded_holiday_dates.add(expanded_date.strftime(\"%Y-%m-%d\"))\n",
    "        \n",
    "    # Step 3: Broadcast the holiday list for efficient lookup\n",
    "    holidays_broadcast = spark.sparkContext.broadcast(expanded_holiday_dates)\n",
    "    \n",
    "    # Step 4: Add 'IS_HOLIDAY' column.\n",
    "    # Convert FL_DATE to \"yyyy-MM-dd\" string, then check if that string is in the broadcasted holiday set\n",
    "    df = df.withColumn(\n",
    "        \"IS_HOLIDAY\",\n",
    "        date_format(col(\"FL_DATE\"), \"yyyy-MM-dd\").isin(holidays_broadcast.value).cast(\"string\")\n",
    "    )\n",
    "\n",
    "    #Step 5: add YEAR:\n",
    "    df = df.withColumn(\"YEAR\", year(col(\"FL_DATE\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def transform_features(df):\n",
    "    df =  df.withColumn(\n",
    "            \"HourlyPrecipitation\",\n",
    "            when(col(\"HourlyPrecipitation\") == \"T\", 0)  # Replace 'T' with 0\n",
    "            .otherwise(regexp_replace(col(\"HourlyPrecipitation\"), \"s\", \"\"))  # Remove 's'\n",
    "            .cast(\"float\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"HourlyWindDirection\", col(\"HourlyWindDirection\").cast(\"float\"))\n",
    "    df =  df.withColumn(\"HourlyPressureChange\", col(\"HourlyPressureChange\").cast(\"float\"))\n",
    "\n",
    "\n",
    "    df =  df.withColumn(\n",
    "            \"HourlyWindSpeed\",\n",
    "            when(col(\"HourlyWindSpeed\") == \"*\", 0)  # Replace '*' with 0\n",
    "            .otherwise(\n",
    "                when(col(\"HourlyWindSpeed\").contains(\"s\"), regexp_replace(col(\"HourlyWindSpeed\"), \"s\", \"\"))\n",
    "                .otherwise(regexp_replace(col(\"HourlyWindSpeed\"), \"V\", \"\"))\n",
    "            )\n",
    "            .cast(\"float\")\n",
    "    )\n",
    "\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyWetBulbTemperature\",\n",
    "        when(col(\"HourlyWetBulbTemperature\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyWetBulbTemperature\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyWetBulbTemperature\").contains(\"s\"), regexp_replace(col(\"HourlyWetBulbTemperature\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyWetBulbTemperature\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyVisibility\",\n",
    "        when(col(\"HourlyVisibility\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyVisibility\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyVisibility\").contains(\"s\"), regexp_replace(col(\"HourlyVisibility\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyVisibility\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyStationPressure\",\n",
    "        when(col(\"HourlyStationPressure\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyStationPressure\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyStationPressure\").contains(\"s\"), regexp_replace(col(\"HourlyStationPressure\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyStationPressure\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyDewPointTemperature\",\n",
    "        when(col(\"HourlyDewPointTemperature\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyDewPointTemperature\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyDewPointTemperature\").contains(\"s\"), regexp_replace(col(\"HourlyDewPointTemperature\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyDewPointTemperature\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyAltimeterSetting\",\n",
    "        when(col(\"HourlyAltimeterSetting\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyAltimeterSetting\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyAltimeterSetting\").contains(\"s\"), regexp_replace(col(\"HourlyAltimeterSetting\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyAltimeterSetting\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyDryBulbTemperature\",\n",
    "        when(col(\"HourlyDryBulbTemperature\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyDryBulbTemperature\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyDryBulbTemperature\").contains(\"s\"), regexp_replace(col(\"HourlyDryBulbTemperature\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyDryBulbTemperature\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df =  df.withColumn(\n",
    "        \"HourlyRelativeHumidity\",\n",
    "        when(col(\"HourlyRelativeHumidity\") == \"*\", 0)  # Replace '*' with 0\n",
    "        .when(col(\"HourlyRelativeHumidity\") == \"T\", 0)  # Replace 'T' with 0\n",
    "        .otherwise(\n",
    "            when(col(\"HourlyRelativeHumidity\").contains(\"s\"), regexp_replace(col(\"HourlyRelativeHumidity\"), \"s\", \"\"))\n",
    "            .otherwise(regexp_replace(col(\"HourlyRelativeHumidity\"), \"V\", \"\"))\n",
    "        )\n",
    "        .cast(\"float\")  # Convert to float\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"sched_depart_unix\", unix_timestamp(\"sched_depart_date_time_UTC\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_time_based_feature(df):\n",
    "    # create a \"year_month\" column (e.g., \"2017-05\")\n",
    "    df = df.withColumn(\"year_month\", date_format(col(\"FL_DATE\"), \"yyyy-MM\"))\n",
    "\n",
    "    # aggregate count of DELAY_TARGET = 1 per carrier per month\n",
    "    monthly_delay_counts = (\n",
    "        df.filter(col(\"DELAY_TARGET\") == 1)\n",
    "        .groupBy(\"OP_UNIQUE_CARRIER\", \"year_month\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"monthly_delay_count\")\n",
    "    )\n",
    "\n",
    "    # define a window of the last 3 months **ordered by year_month**\n",
    "    window_spec = (\n",
    "        Window\n",
    "        .partitionBy(\"OP_UNIQUE_CARRIER\")\n",
    "        .orderBy(\"year_month\")\n",
    "        .rowsBetween(-3, -1)\n",
    "    )\n",
    "\n",
    "    # compute rolling sum of delay counts\n",
    "    rolling_delays = monthly_delay_counts.withColumn(\n",
    "        \"rolling_3mo_delay_count\",\n",
    "        sum(\"monthly_delay_count\").over(window_spec)\n",
    "    )\n",
    "\n",
    "    df_with_rolling = df.join(\n",
    "        rolling_delays,\n",
    "        on=[\"OP_UNIQUE_CARRIER\", \"year_month\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return df_with_rolling\n",
    "\n",
    "def subset_columns(df):\n",
    "    columns_to_keep=[\"QUARTER\",  \"DAY_OF_MONTH\",  \"DAY_OF_WEEK\",  \"OP_UNIQUE_CARRIER\",  \"OP_CARRIER_FL_NUM\",  \"ORIGIN\",  \"DEST\",  \"CRS_DEP_TIME\",  \"DEP_TIME\",  \"DEP_DELAY\",  \"TAXI_OUT\",  \"WHEELS_OFF\",  \"WHEELS_ON\",  \"TAXI_IN\",  \"CRS_ARR_TIME\",  \"CANCELLED\",  \"DIVERTED\",  \"CRS_ELAPSED_TIME\",  \"AIR_TIME\",  \"DISTANCE\",  \"MONTH\",  \"origin_type\",  \"origin_region\",  \"origin_airport_lat\",  \"origin_airport_lon\",  \"dest_type\",  \"dest_region\",  \"dest_airport_lat\",  \"dest_airport_lon\",  \"sched_depart_unix\",  \"ELEVATION\",  \"HourlyAltimeterSetting\",  \"HourlyDewPointTemperature\",  \"HourlyDryBulbTemperature\",  \"HourlyPrecipitation\",  \"HourlyRelativeHumidity\",  \"HourlySkyConditions\",  \"HourlyStationPressure\",  \"HourlyVisibility\",  \"HourlyWetBulbTemperature\",  \"HourlyWindDirection\",  \"HourlyWindSpeed\",  \"DELAY_TARGET\",  \"HAS_WEATHER_TYPE\",  \"IS_HOLIDAY\", \"FL_DATE\", \"rolling_3mo_delay_count\", \"YEAR\"]\n",
    "\n",
    "    return df.select(columns_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca2bfba-1792-43e2-ac87-6353058e0064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60_months = clean_dataset(df_otpw_60_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3b0a7a-ea7b-42c6-85a1-b55de459619b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = df_otpw_60_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c52023-1aa0-4620-b3a4-04253ac089ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_otpw_60_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69faf7f6-8379-44f0-977d-1da0593d425d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@F.udf(returnType=VectorUDT())\n",
    "def array_to_vector_udf(arr):\n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "def time_cv_years(train_df, pipeline):\n",
    "\n",
    "    # Each tuple is (training_upper_bound_year, validation_year).\n",
    "    # For example, (2017, 2018) means:\n",
    "    #   TRAIN: all rows with YEAR <= 2017\n",
    "    #   VALID: all rows with YEAR == 2018\n",
    "    year_folds = [\n",
    "        (2015, 2016),\n",
    "        #(2016, 2017),\n",
    "        (2017, 2018),\n",
    "        # Add more folds as needed.\n",
    "    ]\n",
    "\n",
    "    trainPosPrior = 0.5\n",
    "    realPosPrior = 0.2\n",
    "\n",
    "    # Containers for fold metrics\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_auc = []\n",
    "    all_f1 = []\n",
    "\n",
    "    all_precision_calibrated = []\n",
    "    all_recall_calibrated = []\n",
    "    all_auc_calibrated  = []\n",
    "    all_f1_calibrated = []\n",
    "\n",
    "    # Set up the binary classification evaluator for AUC\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"DELAY_TARGET\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "    # Evaluator that will use the \"calibrated\" probability to measure AUC\n",
    "    evaluator_auc_cal = BinaryClassificationEvaluator(\n",
    "        labelCol=\"DELAY_TARGET\",\n",
    "        rawPredictionCol=\"prob_calibrated_vec\",  # we'll create this column\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "    # We'll filter subsets from the full DataFrame\n",
    "    full_df = train_df\n",
    "\n",
    "\n",
    "    for (train_y, valid_y) in year_folds:\n",
    "        # TRAIN: YEAR <= train_y\n",
    "        train_subset = full_df.filter(F.col(\"YEAR\") <= train_y)\n",
    "\n",
    "        # VALID: YEAR == valid_y\n",
    "        valid_subset = full_df.filter(F.col(\"YEAR\") == valid_y)\n",
    "\n",
    "        # Fit the pipeline on the training subset\n",
    "        pipeline_model = pipeline.fit(train_subset)\n",
    "\n",
    "        # Predict => select minimal columns, then cache\n",
    "        preds = pipeline_model.transform(valid_subset).select(\n",
    "            \"DELAY_TARGET\", \"prediction\", \"rawPrediction\",\n",
    "            \"probability\", \"scaled_features\"\n",
    "        )\n",
    "\n",
    "        preds.cache()\n",
    "\n",
    "        # Compute AUC\n",
    "        auc = evaluator_auc.evaluate(preds)\n",
    "\n",
    "        # Build confusion matrix with pivot\n",
    "        cm = (\n",
    "            preds.groupBy(\"DELAY_TARGET\", \"prediction\")\n",
    "                 .count()\n",
    "        )\n",
    "        pivot_df = (\n",
    "            cm.groupBy(\"DELAY_TARGET\")\n",
    "              .pivot(\"prediction\", [0,1])\n",
    "              .sum(\"count\")\n",
    "              .fillna(0)\n",
    "        )\n",
    "        # Convert pivot result to a dict\n",
    "        dvals = { (r[\"DELAY_TARGET\"], c): r[c] for r in pivot_df.collect() for c in [\"0\",\"1\"] }\n",
    "        def get_count(label, pred):\n",
    "            return dvals.get((label, str(pred)), 0)\n",
    "        tn = get_count(0,0)\n",
    "        fp = get_count(0,1)\n",
    "        fn = get_count(1,0)\n",
    "        tp = get_count(1,1)\n",
    "\n",
    "        # Calculate precision and recall (with safe division)\n",
    "        precision = tp / float(tp + fp) if (tp + fp) else 0.0\n",
    "        recall = tp / float(tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "        f1 = 2.0 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "        # Collect metrics\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        all_auc.append(auc)\n",
    "        all_f1.append(f1)\n",
    "\n",
    "        first_row_vector = preds.select(\"scaled_features\").first()[\"scaled_features\"]\n",
    "        num_features = len(first_row_vector)\n",
    "        num_rows = preds.count()\n",
    "\n",
    "        print(f\"Shape of feature matrix: ({num_rows}, {num_features})\")\n",
    "        print('Non-Calibrated:')\n",
    "        print(f\"Fold: Train <= Year {train_y}, Validate = Year {valid_y} \"\n",
    "              f\"=> Precision={precision:.4f}, Recall={recall:.4f}, AUC={auc:.4f}\")\n",
    "        print(f\"F1={f1:.4f}\")\n",
    "\n",
    "\n",
    "        #Calibrate\n",
    "        # Extract p_raw = probability of positive class\n",
    "        preds_cal = preds.withColumn(\"p_raw\", vector_to_array(F.col(\"probability\"))[1])\n",
    "\n",
    "        preds_cal.cache()\n",
    "        \n",
    "\n",
    "        \n",
    "        #Apply prior correction\n",
    "        p_cal_expr = (\n",
    "            (F.col(\"p_raw\") * (realPosPrior / trainPosPrior))\n",
    "            /\n",
    "            (\n",
    "                (F.col(\"p_raw\") * (realPosPrior / trainPosPrior))\n",
    "                + ((1 - F.col(\"p_raw\")) * ((1 - realPosPrior) / (1 - trainPosPrior)))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        preds_cal = preds_cal.withColumn(\"p_calibrated\", p_cal_expr)\n",
    "\n",
    "        preds_cal = preds_cal.withColumn(\n",
    "            \"prob_calibrated_vec\",\n",
    "            F.array((1 - F.col(\"p_calibrated\")), F.col(\"p_calibrated\"))\n",
    "        )\n",
    "\n",
    "        preds_cal = preds_cal.withColumn(\n",
    "            \"prediction_cal\",\n",
    "            (F.col(\"p_calibrated\") >= 0.3).cast(\"integer\")\n",
    "        )\n",
    "\n",
    "        # **Convert** that array into a Spark ML vector:\n",
    "        preds_cal = preds_cal.withColumn(\n",
    "            \"prob_calibrated_vec\",  \n",
    "            array_to_vector_udf(F.col(\"prob_calibrated_vec\"))\n",
    "        )\n",
    "\n",
    "\n",
    "        #Compute AUC (calibrated)\n",
    "        auc_cal = evaluator_auc_cal.evaluate(preds_cal)\n",
    "\n",
    "\n",
    "        cm_cal = (\n",
    "            preds_cal.groupBy(\"DELAY_TARGET\", \"prediction_cal\")\n",
    "                     .count()\n",
    "        )\n",
    "        pivot_cal = (\n",
    "            cm_cal.groupBy(\"DELAY_TARGET\")\n",
    "                  .pivot(\"prediction_cal\", [0,1])\n",
    "                  .sum(\"count\")\n",
    "                  .fillna(0)\n",
    "        )\n",
    "        dvals_cal = { (r[\"DELAY_TARGET\"], c): r[c] for r in pivot_cal.collect() for c in [\"0\",\"1\"] }\n",
    "        def get_cal(label, pred):\n",
    "            return dvals_cal.get((label, str(pred)), 0)\n",
    "        \n",
    "        tn = get_cal(0,0)\n",
    "        fp = get_cal(0,1)\n",
    "        fn = get_cal(1,0)\n",
    "        tp = get_cal(1,1)\n",
    "        \n",
    "\n",
    "\n",
    "        precision_cal = tp / float(tp + fp) if (tp + fp) else 0.0\n",
    "        recall_cal = tp / float(tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "        f1_cal = 2.0 * precision_cal * recall_cal / (precision_cal + recall_cal) if (precision_cal + recall_cal) else 0.0\n",
    "\n",
    "        all_precision_calibrated.append(precision_cal)\n",
    "        all_recall_calibrated.append(recall_cal)\n",
    "        all_auc_calibrated.append(auc_cal)\n",
    "        all_f1_calibrated.append(f1_cal)\n",
    "\n",
    "        print('Calibrated:')\n",
    "        print(f\"Fold: Train <= {train_y}, Validate = {valid_y}\")\n",
    "        print(f\"   => Shape: ({num_rows}, {num_features})\")\n",
    "        print(f\"   => Precision_cal={precision_cal:.4f}, Recall_cal={recall_cal:.4f}, AUC_cal={auc_cal:.4f}\")\n",
    "        print(f\"   => F1_cal={f1_cal:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        preds_cal.unpersist()\n",
    "        preds.unpersist()\n",
    "\n",
    "    # Aggregate metrics across folds\n",
    "    # Use builtins to prevent conflicts in pySpark SQL sum\n",
    "    avg_precision = builtins.sum(all_precision) / len(all_precision)\n",
    "    avg_recall = builtins.sum(all_recall) / len(all_recall)\n",
    "    avg_auc = builtins.sum(all_auc) / len(all_auc)\n",
    "    avg_f1 = builtins.sum(all_f1) / len(all_f1)\n",
    "\n",
    "    print(\"\\nTime-Series CV Results:\")\n",
    "    print(f\"Avg Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Avg Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"Avg AUC:       {avg_auc:.4f}\")\n",
    "    print(f\"Avg F1:        {avg_f1:.4f}\")\n",
    "\n",
    "    avg_precision_calibrated = builtins.sum(all_precision_calibrated) / len(all_precision_calibrated)\n",
    "    avg_recall_calibrated = builtins.sum(all_recall_calibrated) / len(all_recall_calibrated)\n",
    "    avg_auc_calibrated = builtins.sum(all_auc_calibrated) / len(all_auc_calibrated)\n",
    "    avg_f1_calibrated = builtins.sum(all_f1_calibrated) / len(all_f1_calibrated)\n",
    "\n",
    "    print(\"Calibrated:\")\n",
    "    print(f\"Avg Precision_cal: {avg_precision_calibrated:.4f}\")\n",
    "    print(f\"Avg Recall_cal:    {avg_recall_calibrated:.4f}\")\n",
    "    print(f\"Avg AUC_cal:       {avg_auc_calibrated:.4f}\")\n",
    "    print(f\"Avg F1_cal:        {avg_f1_calibrated:.4f}\")\n",
    "\n",
    "\n",
    "    return avg_precision, avg_recall, avg_auc, avg_f1, avg_precision_calibrated, avg_recall_calibrated, avg_auc_calibrated, avg_f1_calibrated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3528fbb3-cca4-4b80-a694-2e79cc1c7081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def time_cv_years_with_gridsearch(\n",
    "    train_df,\n",
    "    pipeline,\n",
    "    paramGrid,\n",
    "):\n",
    "\n",
    "    best_params = None\n",
    "    best_params_calibrated = None\n",
    "    best_metrics = (0.0, 0.0, 0.0, 0.0)\n",
    "    best_metrics_calibrated = (0.0, 0.0, 0.0, 0.0)\n",
    "    best_auc = float(\"-inf\")\n",
    "    best_auc_calibrated = float(\"-inf\")\n",
    "    best_recall = float(\"-inf\")\n",
    "    best_recall_calibrated = float(\"-inf\")\n",
    "\n",
    "    for i, params in enumerate(paramGrid):\n",
    "        # Make a copy of the pipeline with these param settings\n",
    "        pipeline_with_params = pipeline.copy(params)\n",
    "\n",
    "        # Evaluate with custom CV\n",
    "        avg_precision, avg_recall, avg_auc, avg_f1, avg_precision_calibrated, avg_recall_calibrated, avg_auc_calibrated, avg_f1_calibrated = time_cv_years(\n",
    "            train_df, pipeline_with_params\n",
    "        )\n",
    "\n",
    "        print(f\"[GridSearch] Param Set {i+1}/{len(paramGrid)} Params: {params} => \"\n",
    "              f\"AUC={avg_auc:.4f}, Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, f1={avg_f1:.4f}, Params={params}\")\n",
    "        print()\n",
    "        print(f\"Calibrated: AUC={avg_auc_calibrated:.4f}, Precision={avg_precision_calibrated:.4f}, Recall={avg_recall_calibrated:.4f}, f1={avg_f1_calibrated:.4f}\")\n",
    "\n",
    "        # Decide if this set is better\n",
    "        if avg_recall > best_recall:\n",
    "            best_recall = avg_recall\n",
    "            best_params = params\n",
    "            best_metrics = (avg_precision, avg_recall, avg_auc, avg_f1)\n",
    "\n",
    "        if avg_recall_calibrated > best_recall_calibrated:\n",
    "            best_recall_calibrated = avg_recall_calibrated\n",
    "            best_params_calibrated = params\n",
    "            best_metrics_calibrated = (\n",
    "                avg_precision_calibrated,\n",
    "                avg_recall_calibrated,\n",
    "                avg_auc_calibrated,\n",
    "                avg_f1_calibrated\n",
    "            )\n",
    "    print(f\"[GridSearch] Best Params: {best_params}, AUC={best_auc:.4f}, Precision={best_metrics[0]:.4f}, Recall={best_metrics[1]:.4f}\")\n",
    "\n",
    "    return best_params, best_metrics, best_params_calibrated, best_metrics_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43e6aaa-13c4-47c2-9a4b-e41dc8ab744c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mlp_gridsearch(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    pipeline,\n",
    "    paramGrid\n",
    "    \n",
    "):\n",
    "\n",
    "    best_params = None\n",
    "    best_params_calibrated = None\n",
    "    best_metrics = (0.0, 0.0, 0.0, 0.0)\n",
    "    best_metrics_calibrated = (0.0, 0.0, 0.0, 0.0)\n",
    "    best_auc = float(\"-inf\")\n",
    "    best_auc_calibrated = float(\"-inf\")\n",
    "\n",
    "    for i, params in enumerate(paramGrid):\n",
    "        # Make a copy of the pipeline with these param settings\n",
    "        pipeline_with_params = pipeline.copy(params)\n",
    "\n",
    "        fitted_model = pipeline_with_params.fit(train_df)\n",
    "\n",
    "        precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal = evaluate_ml_model(fitted_model, test_df)\n",
    "\n",
    "\n",
    "        print(f\"[GridSearch] Param Set {i+1}/{len(paramGrid)} Params: {params} => \"\n",
    "              f\"AUC={auc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, f1={f1:.4f}, Params={params}\")\n",
    "        print()\n",
    "        print(f\"Calibrated: AUC={auc_cal:.4f}, Precision={precision_cal:.4f}, Recall={recall_cal:.4f}, f1={f1_cal:.4f}\")\n",
    "\n",
    "        # Decide if this set is better\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = params\n",
    "            best_metrics = (precision, recall, auc, f1)\n",
    "\n",
    "        if auc_cal > best_auc_calibrated:\n",
    "            best_auc_calibrated = auc_cal\n",
    "            best_params_calibrated = params\n",
    "            best_metrics_calibrated = (\n",
    "                precision_cal,\n",
    "                recall_cal,\n",
    "                auc_cal,\n",
    "                f1_cal\n",
    "            )\n",
    "    print(f\"[GridSearch] Best Params: {best_params}, AUC={best_auc:.4f}, Precision={best_metrics[0]:.4f}, Recall={best_metrics[1]:.4f}\")\n",
    "\n",
    "    return best_params, best_metrics, best_params_calibrated, best_metrics_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2338f3b3-9fe7-42a7-9e97-259c3efe8dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_ml_model(fitted_model, test_df):\n",
    "    \n",
    "    trainPosPrior = 0.5\n",
    "    realPosPrior = 0.2\n",
    "\n",
    "\n",
    "    # Set up the binary classification evaluator for AUC\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"DELAY_TARGET\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "    # Evaluator that will use the \"calibrated\" probability to measure AUC\n",
    "    evaluator_auc_cal = BinaryClassificationEvaluator(\n",
    "        labelCol=\"DELAY_TARGET\",\n",
    "        rawPredictionCol=\"prob_calibrated_vec\",  # we'll create this column\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = fitted_model.transform(test_df).select(\n",
    "            \"DELAY_TARGET\", \"prediction\", \"rawPrediction\",\n",
    "            \"probability\", \"scaled_features\"\n",
    "        )\n",
    "    \n",
    "    preds.cache()\n",
    "\n",
    "    # Compute AUC\n",
    "    auc = evaluator_auc.evaluate(preds)\n",
    "\n",
    "    # Compute true positives, false positives, and false negatives\n",
    "    tp = preds.filter((F.col(\"prediction\") == 1) & (F.col(\"DELAY_TARGET\") == 1)).count()\n",
    "    fp = preds.filter((F.col(\"prediction\") == 1) & (F.col(\"DELAY_TARGET\") == 0)).count()\n",
    "    fn = preds.filter((F.col(\"prediction\") == 0) & (F.col(\"DELAY_TARGET\") == 1)).count()\n",
    "\n",
    "    # Calculate precision and recall (with safe division)\n",
    "    precision = tp / float(tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / float(tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "    f1 = 2.0 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "\n",
    "\n",
    "    print('Non-Calibrated:')\n",
    "    print(f\"=> Precision={precision:.4f}, Recall={recall:.4f}, AUC={auc:.4f}\")\n",
    "    print(f\"F1={f1:.4f}\")\n",
    "\n",
    "\n",
    "    #Calibrate\n",
    "    # Extract p_raw = probability of positive class\n",
    "    preds_cal = preds.withColumn(\"p_raw\", vector_to_array(F.col(\"probability\"))[1])\n",
    "    \n",
    "\n",
    "    \n",
    "    #Apply prior correction\n",
    "    p_cal_expr = (\n",
    "        (F.col(\"p_raw\") * (realPosPrior / trainPosPrior))\n",
    "        /\n",
    "        (\n",
    "            (F.col(\"p_raw\") * (realPosPrior / trainPosPrior))\n",
    "            + ((1 - F.col(\"p_raw\")) * ((1 - realPosPrior) / (1 - trainPosPrior)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    preds_cal = preds_cal.withColumn(\"p_calibrated\", p_cal_expr)\n",
    "\n",
    "    preds_cal = preds_cal.withColumn(\n",
    "        \"prob_calibrated_vec\",\n",
    "        F.array((1 - F.col(\"p_calibrated\")), F.col(\"p_calibrated\"))\n",
    "    )\n",
    "\n",
    "    preds_cal = preds_cal.withColumn(\n",
    "        \"prediction_cal\",\n",
    "        (F.col(\"p_calibrated\") >= 0.30).cast(\"integer\")\n",
    "    )\n",
    "\n",
    "    # **Convert** that array into a Spark ML vector:\n",
    "    preds_cal = preds_cal.withColumn(\n",
    "        \"prob_calibrated_vec\",  \n",
    "        array_to_vector_udf(F.col(\"prob_calibrated_vec\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    #Compute AUC (calibrated)\n",
    "    auc_cal = evaluator_auc_cal.evaluate(preds_cal)\n",
    "\n",
    "    # --------------------------\n",
    "    # Compute precision, recall from \"prediction_cal\"\n",
    "    tp = preds_cal.filter((F.col(\"prediction_cal\") == 1) & (F.col(\"DELAY_TARGET\") == 1)).count()\n",
    "    fp = preds_cal.filter((F.col(\"prediction_cal\") == 1) & (F.col(\"DELAY_TARGET\") == 0)).count()\n",
    "    fn = preds_cal.filter((F.col(\"prediction_cal\") == 0) & (F.col(\"DELAY_TARGET\") == 1)).count()\n",
    "\n",
    "    precision_cal = tp / float(tp + fp) if (tp + fp) else 0.0\n",
    "    recall_cal = tp / float(tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "    f1_cal = 2.0 * precision_cal * recall_cal / (precision_cal + recall_cal) if (precision_cal + recall_cal) else 0.0\n",
    "    print('Calibrated:')\n",
    "    print(f\"   => Precision_cal={precision_cal:.4f}, Recall_cal={recall_cal:.4f}, AUC_cal={auc_cal:.4f}\")\n",
    "    print(f\"   => F1_cal={f1_cal:.4f}\")\n",
    "    print()\n",
    "\n",
    "    preds.unpersist()\n",
    "\n",
    "    return precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede0980f-4f03-419c-ab01-fc19d6064d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##To use the time_cv_years_with_gridsearch:\n",
    "Create a paramGrid, MLP Example:\n",
    "paramGrid = (\n",
    "\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(yourmodel.maxIter, [50, 100])\n",
    "    .addGrid(yourmodel.stepSize, [0.01, 0.1])\n",
    "    .addGrid(yourmodel.layers, [\n",
    "        [3, 5, 4, 2],   # 2 hidden layers of size 5 and 4\n",
    "        [3, 8, 2],      # 1 hidden layer of size 8\n",
    "    ])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "best_params, best_metrics, best_params_calibrated, best_metrics_calibrated = time_cv_years_with_gridsearch(\n",
    "\n",
    "    train_df=train_df,\n",
    "    pipeline=pipeline, #pass the pipeline with yur model\n",
    "    paramGrid=paramGrid\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4a5dd5-61da-4d79-8ae8-e97eca14dc37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Baseline Model Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae503ab2-ff7f-42d3-8b8a-367f53a33019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/tmp/train_df_60_months\"\n",
    "train_df = spark.read.parquet(checkpoint_path)\n",
    "train_df = train_df.orderBy(col(\"FL_DATE\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e34235-dca5-4c8d-ad5c-5c8b3c88cef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9851d3b2-d665-4b46-9970-46dfd24c3da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf6f566-2be6-43ec-9c4c-d378ffad87fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n|DELAY_TARGET|   count|\n+------------+--------+\n|           1| 4814891|\n|           0|19464430|\n+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy(\"DELAY_TARGET\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34a3116-7d4e-4339-9ab0-eb87a5dec8d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Umesh - adding the code to load test_df\n",
    "checkpoint_path_test = \"/tmp/test_df_60_months\"\n",
    "test_df = spark.read.parquet(checkpoint_path_test)\n",
    "test_df = test_df.dropna()\n",
    "test_df = test_df.orderBy(col(\"FL_DATE\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13af632-2250-4912-9120-2a30ff5fe9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44830b32-85c8-4b32-a602-d2c7906d7bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6009508, 48)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_df.count(), len(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5569ef49-9268-4031-8b04-21b1970af93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(24279321, 48)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df.count(), len(train_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295881cf-8c79-4420-baec-ddec1b6d9feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DELAY_TARGET</th><th>count_delay_target</th><th>ratio</th></tr></thead><tbody><tr><td>1</td><td>4814891</td><td>0.19831242397594231</td></tr><tr><td>0</td><td>19464430</td><td>0.8016875760240577</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         4814891,
         0.19831242397594231
        ],
        [
         0,
         19464430,
         0.8016875760240577
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DELAY_TARGET",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count_delay_target",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ratio",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio_df = train_df.groupBy(\"DELAY_TARGET\").count()\n",
    "ratio_df = ratio_df.withColumnRenamed(\"count\", \"count_delay_target\")\n",
    "total_count = ratio_df.agg({\"count_delay_target\": \"sum\"}).collect()[0][0]\n",
    "ratio_df = ratio_df.withColumn(\"ratio\", ratio_df[\"count_delay_target\"] / total_count)\n",
    "display(ratio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264eb3ec-f119-412d-ac55-fb962a8093e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create baseline predictions (predict \"on time\" 0 with N% probability)\n",
    "train_df = train_df.withColumn(\"baseline_prediction\", when(rand() <= ratio_df.filter(col(\"DELAY_TARGET\") == 0).collect()[0][\"ratio\"], 0).otherwise(1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Prepare data for evaluation\n",
    "assembler = VectorAssembler(inputCols=[\"baseline_prediction\"], outputCol=\"feature_cols_3\")\n",
    "train_df = assembler.transform(train_df)\n",
    "\n",
    "# Evaluate baseline model using AUC\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    rawPredictionCol=\"baseline_prediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = evaluator.evaluate(train_df)\n",
    "\n",
    "# Calculate precision and recall\n",
    "tp = train_df.filter((train_df.DELAY_TARGET == 1) & (train_df.baseline_prediction == 1)).count()\n",
    "fp = train_df.filter((train_df.DELAY_TARGET == 0) & (train_df.baseline_prediction == 1)).count()\n",
    "fn = train_df.filter((train_df.DELAY_TARGET == 1) & (train_df.baseline_prediction == 0)).count()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea656244-53e0-4db3-a47d-2dd18fbda84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.2, 0.2, 0.5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basline metrics\n",
    "precision = round(tp / (tp + fp) if (tp + fp) > 0 else 0.0, 2)\n",
    "recall = round(tp / (tp + fn) if (tp + fn) > 0 else 0.0, 2)\n",
    "auc = round(auc, 2)\n",
    "precision, recall, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4afbc19-b1c3-41ff-a4ab-0abc8fba4883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Absolute Baseline\n",
    "precision = 0.2, recall = 0.2, auc = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a5931b-94fe-4bc2-82b0-ec4f21a9fbf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def downsample(df):\n",
    "    df = df.orderBy(col(\"FL_DATE\").asc())\n",
    "    df_minority = df.filter(df.DELAY_TARGET == 1)\n",
    "    count_minority = df.filter(col(\"DELAY_TARGET\") == 1).count()\n",
    "    df_majority_downsampled = df.filter(col(\"DELAY_TARGET\") == 0).sample(fraction=count_minority / df.filter(col(\"DELAY_TARGET\") == 0).count(), seed=42)\n",
    "    df_downsampled = df_majority_downsampled.union(df_minority)\n",
    "    df_downsampled = df_downsampled.orderBy(\"FL_DATE\")\n",
    "    return df_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be21e0cc-5950-47b1-9149-d0c931e336f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "train_df = downsample(train_df)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35362a11-a6d0-40d4-8e91-c6f9a4f7f917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n|DELAY_TARGET|  count|\n+------------+-------+\n|           0|3733373|\n|           1|3731522|\n+------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy(\"DELAY_TARGET\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5133ec6-a91a-4163-8486-30a71e7335f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_cast = [\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"origin_airport_lat\", \"origin_airport_lon\", \n",
    "    \"dest_airport_lat\", \"dest_airport_lon\", \"ELEVATION\",\n",
    "    \"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyPrecipitation\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyRelativeHumidity\", \"HourlyStationPressure\",\n",
    "    \"HourlyVisibility\", \"HourlyWetBulbTemperature\", \"HourlyWindDirection\",\n",
    "    \"HourlyWindSpeed\", \"rolling_3mo_delay_count\", \"DAY_OF_MONTH\", \n",
    "]\n",
    "\n",
    "for col_name in cols_to_cast:\n",
    "    train_df = train_df.withColumn(col_name, col(col_name).cast(\"float\"))\n",
    "\n",
    "for col_name in cols_to_cast:\n",
    "    test_df = test_df.withColumn(col_name, col(col_name).cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9028681-6d9d-443b-82b5-31f98dde2ab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "categorical_cols = [\n",
    "  \"OP_UNIQUE_CARRIER\", \"origin_type\", \"dest_type\",\n",
    "    \"QUARTER\", \"IS_HOLIDAY\", \"YEAR\", \"MONTH\", \"HAS_WEATHER_TYPE\", \"DAY_OF_WEEK\"\n",
    "]\n",
    "\n",
    "# Numerical columns (removed the duplicate \"HourlyPrecipitation\")\n",
    "numerical_cols = [\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"origin_airport_lat\", \"origin_airport_lon\", \n",
    "    \"dest_airport_lat\", \"dest_airport_lon\", \"sched_depart_unix\", \"ELEVATION\",\n",
    "    \"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyPrecipitation\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyRelativeHumidity\", \"HourlyStationPressure\",\n",
    "    \"HourlyVisibility\", \"HourlyWetBulbTemperature\", \"HourlyWindDirection\",\n",
    "    \"HourlyWindSpeed\", \"rolling_3mo_delay_count\", \"DAY_OF_MONTH\", \n",
    "]\n",
    "\n",
    "\n",
    "# 1) StringIndex each categorical column\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# 2) One-Hot Encode each indexed column\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_ohe\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# 3) VectorAssembler for all features (OHE + numeric)\n",
    "feature_cols = [f\"{col}_ohe\" for col in categorical_cols] + numerical_cols\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"assembled_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 4) MinMaxScaler on assembled_features -> scaled_features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"assembled_features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef16cb36-e7ff-4712-88e5-33bd7646e8c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    predictionCol=\"prediction\",         \n",
    "    rawPredictionCol=\"rawPrediction\",   \n",
    "    probabilityCol=\"probability\"    \n",
    ")\n",
    "\n",
    "baseline_pipeline = Pipeline(stages=indexers + encoders + [vector_assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4aa3414-fe15-4a18-a903-87e1e7ce3682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training Data Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26827a56-8470-45a8-9187-d7f2ada05ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5ce2c17ab340bc9e5cfa9585204c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a503f071bf1248da945003834a687c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696836, 79)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5961, Recall=0.5778, AUC=0.6468\nF1=0.5868\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696836, 79)\n   => Precision_cal=0.6657, Recall_cal=0.2351, AUC_cal=0.6468\n   => F1_cal=0.3475\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37319f4a8414687b80c0d4768927db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa09ac97e357412da8559911461cc410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2226090, 81)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6068, Recall=0.6425, AUC=0.6433\nF1=0.6241\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226090, 81)\n   => Precision_cal=0.6813, Recall_cal=0.2863, AUC_cal=0.6433\n   => F1_cal=0.4032\n\n\nTime-Series CV Results:\nAvg Precision: 0.6014\nAvg Recall:    0.6101\nAvg AUC:       0.6451\nAvg F1:        0.6055\nCalibrated:\nAvg Precision_cal: 0.6735\nAvg Recall_cal:    0.2607\nAvg AUC_cal:       0.6451\nAvg F1_cal:        0.3754\n"
     ]
    }
   ],
   "source": [
    "avg_precision, avg_recall, avg_auc, avg_f1, avg_precision_calibrated, avg_recall_calibrated, avg_auc_calibrated, avg_f1_calibrated = time_cv_years(train_df, baseline_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae0e2da-d41e-4c64-9714-2f2718f99e83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Threshold of 0.4\n",
    "\n",
    "Shape of feature matrix: (2223217, 82)\n",
    "Non-Calibrated:\n",
    "Fold: Train <= Year 2017, Validate = Year 2018 \n",
    "- Precision=0.6582\n",
    "- Recall=0.6754\n",
    "- AUC=0.7105\n",
    "- F1=0.6667\n",
    "\n",
    "Calibrated:\n",
    "Fold: Train <= 2017, Validate = 2018\n",
    "Shape: (2223217, 82)\n",
    "- Precision_cal=0.8729\n",
    "- Recall_cal=0.1499\n",
    "- AUC_cal=0.7105\n",
    "- F1_cal=0.2559\n",
    "\n",
    "\n",
    "Time-Series CV Results:\n",
    "- Avg Precision: 0.6779\n",
    "- Avg Recall:    0.6630\n",
    "- Avg AUC:       0.7343\n",
    "- Avg F1:        0.6700\n",
    "Calibrated:\n",
    "- Avg Precision_cal: 0.9008\n",
    "- Avg Recall_cal:    0.1599\n",
    "- Avg AUC_cal:       0.7343\n",
    "- Avg F1_cal:        0.2716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8bf2c0-8cd5-4f97-9b00-a4c28601e5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (Final) Threshold of 0.3\n",
    "Shape of feature matrix: (1697609, 80)\n",
    "Non-Calibrated:\n",
    "Fold: Train <= Year 2015, Validate = Year 2016 => \n",
    "- Precision=0.6976\n",
    "- Recall=0.6495\n",
    "- AUC=0.7575\n",
    "- F1=0.6727\n",
    "\n",
    "Calibrated:\n",
    "Fold: Train <= 2015, Validate = 2016\n",
    "   => Shape: (1697609, 80)\n",
    "- Precision_cal=0.8359\n",
    "- Recall_cal=0.3563\n",
    "- AUC_cal=0.7575\n",
    "- F1_cal=0.4996\n",
    "\n",
    "\n",
    "Shape of feature matrix: (2223294, 82)\n",
    "Non-Calibrated:\n",
    "Fold: Train <= Year 2017, Validate = Year 2018 => \n",
    "- Precision=0.6568\n",
    "- Recall=0.6784\n",
    "- AUC=0.7100\n",
    "- F1=0.6674\n",
    "\n",
    "Calibrated:\n",
    "Fold: Train <= 2017, Validate = 2018\n",
    "   => Shape: (2223294, 82)\n",
    "- Precision_cal=0.7727\n",
    "- Recall_cal=0.3657\n",
    "- AUC_cal=0.7100\n",
    "- F1_cal=0.4964\n",
    "\n",
    "### Overall Results\n",
    "\n",
    "Time-Series CV Results:\n",
    "- Avg Precision: 0.6772\n",
    "- Avg Recall:    0.6640\n",
    "- Avg AUC:       0.7338\n",
    "- Avg F1:        0.6701\n",
    "\n",
    "Calibrated:\n",
    "- Avg Precision_cal: 0.8043\n",
    "- Avg Recall_cal:    0.3610\n",
    "- Avg AUC_cal:       0.7338\n",
    "- Avg F1_cal:        0.4980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2080f98e-b5e0-4517-bb77-8dcae7a94668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Data Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a177a85-ce0d-4553-b990-5deb2720d525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d5126b-7bfe-4eff-919d-09e167be4d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73f93fbe6804fbbbbca4440723d5c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3954f1534600456b8855b457f9ebd8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_test = baseline_pipeline.fit(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0763bf8d-0413-4953-a54b-ec3fe48ce04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Calibrated:\n=> Precision=0.2633, Recall=0.6461, AUC=0.6500\nF1=0.3742\nCalibrated:\n   => Precision_cal=0.3367, Recall_cal=0.2872, AUC_cal=0.6500\n   => F1_cal=0.3100\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.2633433233527429,\n",
       " 0.6461126586813776,\n",
       " 0.649989166190065,\n",
       " 0.3741785378482383,\n",
       " 0.3367230178838238,\n",
       " 0.28720234093257324,\n",
       " 0.6499892675496601,\n",
       " 0.309997462406631)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal = evaluate_ml_model(baseline_test, test_df)\n",
    "precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6adf0f97-ab62-4cc8-8b03-8750bae01d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Threshold At 0.4\n",
    "Non-Calibrated:\n",
    "=> Precision=0.2998, Recall=0.7073, AUC=0.7171\n",
    "F1=0.4211\n",
    "Calibrated:\n",
    "   => Precision_cal=0.6132, Recall_cal=0.1676, AUC_cal=0.7171\n",
    "   => F1_cal=0.2632\n",
    "\n",
    "(0.2998188895446865,\n",
    " 0.7073010133573002,\n",
    " 0.7170892944373173,\n",
    " 0.4211260323374925,\n",
    " 0.6132276106374469,\n",
    " 0.16759737764194246,\n",
    " 0.7170893706232652,\n",
    " 0.26324807987240256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d02eb55-adfa-49f0-8559-95c9d8733161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (Final) Threshold At 0.30\n",
    "Non-Calibrated:\n",
    "- Precision=0.2998\n",
    "- Recall=0.7073\n",
    "- AUC=0.7171\n",
    "- F1=0.4211 \n",
    "\n",
    "Calibrated: \n",
    "- Precision_cal=0.6132\n",
    "- Recall_cal=0.1676\n",
    "- AUC_cal=0.7171\n",
    "- F1_cal=0.2632\n",
    "\n",
    "(0.29980609442716266,\n",
    " 0.7076809898126404,\n",
    " 0.7171843933165093,\n",
    " 0.4211807316937796,\n",
    " 0.43505609073574203,\n",
    " 0.39625818619366826,\n",
    " 0.7171841548987117,\n",
    " 0.41475177845909095)\n",
    " 0.41451334958181213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f496da1-aa14-45eb-b3cd-b0a75f916fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sorted Logistic Regression Coefficients (Absolute Basis) ===\n\nDAY_OF_MONTH                             -> 6.329973\nHourlyWindDirection                      -> -2.430201\nrolling_3mo_delay_count                  -> -1.496087\nELEVATION                                -> 1.268183\nHourlyDryBulbTemperature                 -> -1.079132\nHourlyAltimeterSetting                   -> 0.674307\nHourlyDewPointTemperature                -> 0.627263\nHourlyWindSpeed                          -> -0.553703\nOP_UNIQUE_CARRIER = HA                   -> -0.505725\nOP_UNIQUE_CARRIER = F9                   -> 0.430347\nOP_UNIQUE_CARRIER = DL                   -> -0.429041\nOP_UNIQUE_CARRIER = B6                   -> 0.362700\nDAY_OF_WEEK = 1                          -> -0.319243\nOP_UNIQUE_CARRIER = G4                   -> 0.310929\nMONTH = 4                                -> 0.290975\nMONTH = 1                                -> 0.290460\nOP_UNIQUE_CARRIER = VX                   -> 0.257529\nOP_UNIQUE_CARRIER = NK                   -> 0.250698\nDAY_OF_WEEK = 4                          -> -0.224935\nOP_UNIQUE_CARRIER = AS                   -> -0.219679\nMONTH = 10                               -> 0.211596\nHourlyVisibility                         -> 0.204201\nOP_UNIQUE_CARRIER = YV                   -> 0.194782\nOP_UNIQUE_CARRIER = MQ                   -> 0.191491\nOP_UNIQUE_CARRIER = OH                   -> 0.190891\ndest_airport_lon                         -> -0.175989\nDAY_OF_WEEK = 5                          -> -0.164729\nHAS_WEATHER_TYPE = 0                     -> -0.158677\nQUARTER = 2                              -> -0.135820\nOP_UNIQUE_CARRIER = EV                   -> 0.132400\nCRS_ARR_TIME                             -> 0.127882\nDAY_OF_WEEK = 6                          -> 0.126930\nDAY_OF_WEEK = 7                          -> -0.126930\nIS_HOLIDAY = true                        -> -0.120196\nCRS_ELAPSED_TIME                         -> 0.115596\nQUARTER = 1                              -> 0.109472\nHourlyPrecipitation                      -> -0.099029\ndest_type = small_airport                -> 0.089026\nQUARTER = 3                              -> -0.087483\nDAY_OF_WEEK = 3                          -> -0.084832\nMONTH = 2                                -> 0.078135\nOP_UNIQUE_CARRIER = OO                   -> 0.074571\nHourlyWetBulbTemperature                 -> 0.071811\norigin_airport_lon                       -> -0.069105\nHourlyStationPressure                    -> -0.067184\nDISTANCE                                 -> 0.067040\norigin_airport_lat                       -> -0.063012\norigin_type = small_airport              -> -0.062569\norigin_type = medium_airport             -> 0.062198\nOP_UNIQUE_CARRIER = 9E                   -> 0.058745\nIS_HOLIDAY = false                       -> 0.057317\nYEAR = 2018                              -> -0.056295\ndest_airport_lat                         -> -0.055219\nMONTH = 8                                -> 0.048513\nYEAR = 2017                              -> -0.047913\nYEAR = 2016                              -> 0.047913\nMONTH = 3                                -> -0.040066\nHAS_WEATHER_TYPE = 1                     -> -0.035373\nOP_UNIQUE_CARRIER = YX                   -> -0.032039\nOP_UNIQUE_CARRIER = UA                   -> -0.031201\ndest_type = large_airport                -> -0.028091\nMONTH = 9                                -> -0.024344\nMONTH = 11                               -> -0.020816\nOP_UNIQUE_CARRIER = AA                   -> 0.017118\nOP_UNIQUE_CARRIER = WN                   -> 0.017014\nHourlyRelativeHumidity                   -> 0.012901\nMONTH = 5                                -> -0.011387\nOP_UNIQUE_CARRIER = US                   -> -0.001361\nMONTH = 6                                -> 0.000952\norigin_type = large_airport              -> 0.000000\ndest_type = medium_airport               -> 0.000000\nQUARTER = 4                              -> 0.000000\nYEAR = 2015                              -> 0.000000\nMONTH = 7                                -> 0.000000\nMONTH = 12                               -> 0.000000\nDAY_OF_WEEK = 2                          -> 0.000000\nCRS_DEP_TIME                             -> 0.000000\nsched_depart_unix                        -> 0.000000\n\nIntercept: 2.043714\n"
     ]
    }
   ],
   "source": [
    "# Extract pipeline stages\n",
    "assembler = [s for s in baseline_test.stages if isinstance(s, VectorAssembler)][0]\n",
    "ohe_models = [s for s in baseline_test.stages if isinstance(s, OneHotEncoderModel)]\n",
    "indexer_models = [s for s in baseline_test.stages if isinstance(s, StringIndexerModel)]\n",
    "\n",
    "# Build mappings\n",
    "ohe_output_sizes = {}\n",
    "ohe_input_output_map = {}\n",
    "for ohe in ohe_models:\n",
    "    out_col = ohe.getOutputCol()\n",
    "    in_col = ohe.getInputCol()\n",
    "    size = ohe.categorySizes[0]\n",
    "    ohe_output_sizes[out_col] = size\n",
    "    ohe_input_output_map[out_col] = in_col\n",
    "\n",
    "indexer_label_dict = {\n",
    "    indexer.getOutputCol(): indexer.labels for indexer in indexer_models\n",
    "}\n",
    "\n",
    "# Get coefficients\n",
    "lr_model = baseline_test.stages[-1]\n",
    "coefficients = lr_model.coefficients\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "# Collect featureâcoefficient pairs\n",
    "feature_coeff_pairs = []\n",
    "current_index = 0\n",
    "\n",
    "for col in assembler.getInputCols():\n",
    "    if col in numerical_cols:\n",
    "        coef = coefficients[current_index]\n",
    "        feature_coeff_pairs.append((col, coef))\n",
    "        current_index += 1\n",
    "\n",
    "    elif col in ohe_output_sizes:\n",
    "        size = ohe_output_sizes[col]\n",
    "        num_dims = size - 1  # default: dropLast=True\n",
    "\n",
    "        index_col = ohe_input_output_map[col]\n",
    "        labels = indexer_label_dict.get(index_col, [f\"{col}_cat_{i}\" for i in range(num_dims)])\n",
    "        labels = labels[:num_dims]  # due to dropLast=True\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            coef = coefficients[current_index]\n",
    "            feature_label = f\"{col.replace('_ohe', '')} = {label}\"\n",
    "            feature_coeff_pairs.append((feature_label, coef))\n",
    "            current_index += 1\n",
    "    else:\n",
    "        feature_coeff_pairs.append((f\"UNKNOWN: {col}\", coefficients[current_index]))\n",
    "        current_index += 1\n",
    "\n",
    "# Sort by absolute value of coefficient\n",
    "sorted_features = sorted(feature_coeff_pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Sorted Logistic Regression Coefficients (Absolute Basis) ===\\n\")\n",
    "for feature, coef in sorted_features:\n",
    "    print(f\"{feature:40s} -> {coef:.6f}\")\n",
    "\n",
    "print(f\"\\nIntercept: {intercept:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a03cecc-a87c-4125-acc8-482544d0b361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f96619-3916-48d9-a72d-e0761a120411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##MLP Single Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c60aec6-0a87-42bb-99f2-9b81fe15d052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Anshul\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b2b8de-71fd-4a58-89c4-85b142ab7249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the vector assembler \n",
    "feature_cols = [f\"{col}_index\" for col in categorical_cols] + numerical_cols\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"assembled_features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f57163-87f7-4d13-9bf8-66682afb49bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MinMaxScaler - same as random forest\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"assembled_features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abaa20eb-858a-42d6-9212-5f0e42e2f40a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1640fcec68e24129aed73918a83d7c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1765a005e6384719bc4e2dc4083c1a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector size: 31\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline to determine the vector size\n",
    "vector_size_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler])\n",
    "vector_size_model = vector_size_pipeline.fit(train_df)\n",
    "vector_size_df = vector_size_model.transform(train_df)\n",
    "\n",
    "# Get the vector size from the first row\n",
    "first_row = vector_size_df.select(\"scaled_features\").first()\n",
    "vector_size = len(first_row[\"scaled_features\"])\n",
    "print(f\"Input vector size: {vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7180bb19-0881-4c79-899d-52c17f9eae34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "MLP = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    predictionCol=\"prediction\",         \n",
    "    rawPredictionCol=\"rawPrediction\",   \n",
    "    probabilityCol=\"probability\" ,\n",
    "    solver=\"l-bfgs\",\n",
    "    layers=[vector_size, 20, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ca7a79-e37d-4ec0-960a-d79b50a21754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler, MLP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b81c095-f222-4ef3-81c7-5f9d64ef9767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "layer_configs = [\n",
    "    [vector_size, 10, 2],  # Input layer, hidden layer with 10 neurons, output layer\n",
    "    [vector_size, 20, 2],  # Input layer, hidden layer with 20 neurons, output layer\n",
    "    [vector_size, 30, 2]   # Input layer, hidden layer with 30 neurons, output layer\n",
    "]\n",
    "\n",
    "# Create parameter grid\n",
    "mlp_paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(MLP.layers, layer_configs)\n",
    "    .addGrid(MLP.maxIter, [25, 50, 100])\n",
    "    .addGrid(MLP.stepSize, [0.01, 0.05])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d25c19-0e36-476f-bb01-89ecb0509131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696903, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5887, Recall=0.5671, AUC=0.6344\nF1=0.5777\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696903, 31)\n   => Precision_cal=0.6636, Recall_cal=0.1865, AUC_cal=0.6388\n   => F1_cal=0.2911\n\nShape of feature matrix: (2225943, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6132, Recall=0.5563, AUC=0.6364\nF1=0.5833\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225943, 31)\n   => Precision_cal=0.6979, Recall_cal=0.1761, AUC_cal=0.6343\n   => F1_cal=0.2813\n\n\nTime-Series CV Results:\nAvg Precision: 0.6009\nAvg Recall:    0.5617\nAvg AUC:       0.6354\nAvg F1:        0.5805\nCalibrated:\nAvg Precision_cal: 0.6807\nAvg Recall_cal:    0.1813\nAvg AUC_cal:       0.6365\nAvg F1_cal:        0.2862\n[GridSearch] Param Set 1/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6354, Precision=0.6009, Recall=0.5617, f1=0.5805, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6365, Precision=0.6807, Recall=0.1813, f1=0.2862\nShape of feature matrix: (1696642, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5858, Recall=0.6010, AUC=0.6373\nF1=0.5933\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696642, 31)\n   => Precision_cal=0.6640, Recall_cal=0.2034, AUC_cal=0.6412\n   => F1_cal=0.3115\n\nShape of feature matrix: (2225949, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6137, Recall=0.5557, AUC=0.6366\nF1=0.5833\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225949, 31)\n   => Precision_cal=0.6963, Recall_cal=0.1812, AUC_cal=0.6345\n   => F1_cal=0.2876\n\n\nTime-Series CV Results:\nAvg Precision: 0.5997\nAvg Recall:    0.5783\nAvg AUC:       0.6369\nAvg F1:        0.5883\nCalibrated:\nAvg Precision_cal: 0.6801\nAvg Recall_cal:    0.1923\nAvg AUC_cal:       0.6379\nAvg F1_cal:        0.2995\n[GridSearch] Param Set 2/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6369, Precision=0.5997, Recall=0.5783, f1=0.5883, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6379, Precision=0.6801, Recall=0.1923, f1=0.2995\nShape of feature matrix: (1696906, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6343, Recall=0.2937, AUC=0.6309\nF1=0.4015\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696906, 31)\n   => Precision_cal=0.7002, Recall_cal=0.0634, AUC_cal=0.6336\n   => F1_cal=0.1163\n\nShape of feature matrix: (2225956, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6015, Recall=0.6268, AUC=0.6400\nF1=0.6139\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225956, 31)\n   => Precision_cal=0.6857, Recall_cal=0.2631, AUC_cal=0.6377\n   => F1_cal=0.3803\n\n\nTime-Series CV Results:\nAvg Precision: 0.6179\nAvg Recall:    0.4603\nAvg AUC:       0.6354\nAvg F1:        0.5077\nCalibrated:\nAvg Precision_cal: 0.6929\nAvg Recall_cal:    0.1633\nAvg AUC_cal:       0.6356\nAvg F1_cal:        0.2483\n[GridSearch] Param Set 3/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6354, Precision=0.6179, Recall=0.4603, f1=0.5077, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6356, Precision=0.6929, Recall=0.1633, f1=0.2483\nShape of feature matrix: (1696873, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6120, Recall=0.4055, AUC=0.6326\nF1=0.4878\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696873, 31)\n   => Precision_cal=0.6880, Recall_cal=0.0934, AUC_cal=0.6344\n   => F1_cal=0.1644\n\nShape of feature matrix: (2225930, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6003, Recall=0.6270, AUC=0.6392\nF1=0.6134\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225930, 31)\n   => Precision_cal=0.6929, Recall_cal=0.2306, AUC_cal=0.6365\n   => F1_cal=0.3461\n\n\nTime-Series CV Results:\nAvg Precision: 0.6062\nAvg Recall:    0.5163\nAvg AUC:       0.6359\nAvg F1:        0.5506\nCalibrated:\nAvg Precision_cal: 0.6904\nAvg Recall_cal:    0.1620\nAvg AUC_cal:       0.6355\nAvg F1_cal:        0.2553\n[GridSearch] Param Set 4/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6359, Precision=0.6062, Recall=0.5163, f1=0.5506, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6355, Precision=0.6904, Recall=0.1620, f1=0.2553\nShape of feature matrix: (1696660, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5919, Recall=0.5517, AUC=0.6352\nF1=0.5711\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696660, 31)\n   => Precision_cal=0.6394, Recall_cal=0.2726, AUC_cal=0.6377\n   => F1_cal=0.3822\n\nShape of feature matrix: (2225979, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6010, Recall=0.6365, AUC=0.6410\nF1=0.6183\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225979, 31)\n   => Precision_cal=0.6905, Recall_cal=0.2728, AUC_cal=0.6402\n   => F1_cal=0.3910\n\n\nTime-Series CV Results:\nAvg Precision: 0.5964\nAvg Recall:    0.5941\nAvg AUC:       0.6381\nAvg F1:        0.5947\nCalibrated:\nAvg Precision_cal: 0.6650\nAvg Recall_cal:    0.2727\nAvg AUC_cal:       0.6389\nAvg F1_cal:        0.3866\n[GridSearch] Param Set 5/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6381, Precision=0.5964, Recall=0.5941, f1=0.5947, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6389, Precision=0.6650, Recall=0.2727, f1=0.3866\nShape of feature matrix: (1696617, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5506, Recall=0.7860, AUC=0.6397\nF1=0.6476\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696617, 31)\n   => Precision_cal=0.6102, Recall_cal=0.5032, AUC_cal=0.6452\n   => F1_cal=0.5516\n\nShape of feature matrix: (2225995, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6072, Recall=0.6146, AUC=0.6428\nF1=0.6109\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225995, 31)\n   => Precision_cal=0.6972, Recall_cal=0.2603, AUC_cal=0.6419\n   => F1_cal=0.3791\n\n\nTime-Series CV Results:\nAvg Precision: 0.5789\nAvg Recall:    0.7003\nAvg AUC:       0.6412\nAvg F1:        0.6292\nCalibrated:\nAvg Precision_cal: 0.6537\nAvg Recall_cal:    0.3818\nAvg AUC_cal:       0.6435\nAvg F1_cal:        0.4653\n[GridSearch] Param Set 6/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6412, Precision=0.5789, Recall=0.7003, f1=0.6292, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6435, Precision=0.6537, Recall=0.3818, f1=0.4653\nShape of feature matrix: (1696790, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6071, Recall=0.4713, AUC=0.6410\nF1=0.5307\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696790, 31)\n   => Precision_cal=0.6874, Recall_cal=0.1201, AUC_cal=0.6384\n   => F1_cal=0.2045\n\nShape of feature matrix: (2226084, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6078, Recall=0.5989, AUC=0.6343\nF1=0.6033\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226084, 31)\n   => Precision_cal=0.6889, Recall_cal=0.2158, AUC_cal=0.6352\n   => F1_cal=0.3287\n\n\nTime-Series CV Results:\nAvg Precision: 0.6074\nAvg Recall:    0.5351\nAvg AUC:       0.6377\nAvg F1:        0.5670\nCalibrated:\nAvg Precision_cal: 0.6881\nAvg Recall_cal:    0.1679\nAvg AUC_cal:       0.6368\nAvg F1_cal:        0.2666\n[GridSearch] Param Set 7/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6377, Precision=0.6074, Recall=0.5351, f1=0.5670, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6368, Precision=0.6881, Recall=0.1679, f1=0.2666\nShape of feature matrix: (1697002, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6011, Recall=0.4997, AUC=0.6407\nF1=0.5458\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1697002, 31)\n   => Precision_cal=0.6775, Recall_cal=0.1446, AUC_cal=0.6379\n   => F1_cal=0.2384\n\nShape of feature matrix: (2226053, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6115, Recall=0.5703, AUC=0.6327\nF1=0.5902\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226053, 31)\n   => Precision_cal=0.6941, Recall_cal=0.1864, AUC_cal=0.6337\n   => F1_cal=0.2938\n\n\nTime-Series CV Results:\nAvg Precision: 0.6063\nAvg Recall:    0.5350\nAvg AUC:       0.6367\nAvg F1:        0.5680\nCalibrated:\nAvg Precision_cal: 0.6858\nAvg Recall_cal:    0.1655\nAvg AUC_cal:       0.6358\nAvg F1_cal:        0.2661\n[GridSearch] Param Set 8/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6367, Precision=0.6063, Recall=0.5350, f1=0.5680, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6358, Precision=0.6858, Recall=0.1655, f1=0.2661\nShape of feature matrix: (1696983, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6227, Recall=0.4136, AUC=0.6440\nF1=0.4970\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696983, 31)\n   => Precision_cal=0.7005, Recall_cal=0.0946, AUC_cal=0.6403\n   => F1_cal=0.1667\n\nShape of feature matrix: (2225952, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6051, Recall=0.6083, AUC=0.6351\nF1=0.6067\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225952, 31)\n   => Precision_cal=0.6802, Recall_cal=0.2704, AUC_cal=0.6357\n   => F1_cal=0.3870\n\n\nTime-Series CV Results:\nAvg Precision: 0.6139\nAvg Recall:    0.5110\nAvg AUC:       0.6395\nAvg F1:        0.5519\nCalibrated:\nAvg Precision_cal: 0.6904\nAvg Recall_cal:    0.1825\nAvg AUC_cal:       0.6380\nAvg F1_cal:        0.2768\n[GridSearch] Param Set 9/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6395, Precision=0.6139, Recall=0.5110, f1=0.5519, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6380, Precision=0.6904, Recall=0.1825, f1=0.2768\nShape of feature matrix: (1696832, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6234, Recall=0.4067, AUC=0.6425\nF1=0.4923\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696832, 31)\n   => Precision_cal=0.7033, Recall_cal=0.0865, AUC_cal=0.6403\n   => F1_cal=0.1540\n\nShape of feature matrix: (2226118, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6023, Recall=0.6329, AUC=0.6362\nF1=0.6172\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226118, 31)\n   => Precision_cal=0.6853, Recall_cal=0.2710, AUC_cal=0.6385\n   => F1_cal=0.3884\n\n\nTime-Series CV Results:\nAvg Precision: 0.6129\nAvg Recall:    0.5198\nAvg AUC:       0.6393\nAvg F1:        0.5548\nCalibrated:\nAvg Precision_cal: 0.6943\nAvg Recall_cal:    0.1788\nAvg AUC_cal:       0.6394\nAvg F1_cal:        0.2712\n[GridSearch] Param Set 10/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6393, Precision=0.6129, Recall=0.5198, f1=0.5548, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6394, Precision=0.6943, Recall=0.1788, f1=0.2712\nShape of feature matrix: (1696889, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5684, Recall=0.6847, AUC=0.6441\nF1=0.6212\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696889, 31)\n   => Precision_cal=0.6274, Recall_cal=0.4033, AUC_cal=0.6435\n   => F1_cal=0.4910\n\nShape of feature matrix: (2226087, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.5957, Recall=0.6684, AUC=0.6338\nF1=0.6300\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226087, 31)\n   => Precision_cal=0.6845, Recall_cal=0.3044, AUC_cal=0.6415\n   => F1_cal=0.4214\n\n\nTime-Series CV Results:\nAvg Precision: 0.5821\nAvg Recall:    0.6766\nAvg AUC:       0.6390\nAvg F1:        0.6256\nCalibrated:\nAvg Precision_cal: 0.6560\nAvg Recall_cal:    0.3539\nAvg AUC_cal:       0.6425\nAvg F1_cal:        0.4562\n[GridSearch] Param Set 11/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6390, Precision=0.5821, Recall=0.6766, f1=0.6256, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6425, Precision=0.6560, Recall=0.3539, f1=0.4562\nShape of feature matrix: (1696952, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5534, Recall=0.7602, AUC=0.6414\nF1=0.6405\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696952, 31)\n   => Precision_cal=0.6104, Recall_cal=0.4905, AUC_cal=0.6454\n   => F1_cal=0.5440\n\nShape of feature matrix: (2226034, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.5987, Recall=0.6507, AUC=0.6372\nF1=0.6236\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226034, 31)\n   => Precision_cal=0.6954, Recall_cal=0.2618, AUC_cal=0.6408\n   => F1_cal=0.3804\n\n\nTime-Series CV Results:\nAvg Precision: 0.5761\nAvg Recall:    0.7055\nAvg AUC:       0.6393\nAvg F1:        0.6321\nCalibrated:\nAvg Precision_cal: 0.6529\nAvg Recall_cal:    0.3761\nAvg AUC_cal:       0.6431\nAvg F1_cal:        0.4622\n[GridSearch] Param Set 12/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6393, Precision=0.5761, Recall=0.7055, f1=0.6321, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6431, Precision=0.6529, Recall=0.3761, f1=0.4622\nShape of feature matrix: (1696786, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6049, Recall=0.4961, AUC=0.6399\nF1=0.5451\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696786, 31)\n   => Precision_cal=0.6808, Recall_cal=0.1521, AUC_cal=0.6395\n   => F1_cal=0.2487\n\nShape of feature matrix: (2225729, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6084, Recall=0.5903, AUC=0.6306\nF1=0.5992\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225729, 31)\n   => Precision_cal=0.6849, Recall_cal=0.2218, AUC_cal=0.6335\n   => F1_cal=0.3351\n\n\nTime-Series CV Results:\nAvg Precision: 0.6066\nAvg Recall:    0.5432\nAvg AUC:       0.6353\nAvg F1:        0.5721\nCalibrated:\nAvg Precision_cal: 0.6828\nAvg Recall_cal:    0.1870\nAvg AUC_cal:       0.6365\nAvg F1_cal:        0.2919\n[GridSearch] Param Set 13/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6353, Precision=0.6066, Recall=0.5432, f1=0.5721, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6365, Precision=0.6828, Recall=0.1870, f1=0.2919\nShape of feature matrix: (1696751, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6087, Recall=0.4752, AUC=0.6394\nF1=0.5338\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696751, 31)\n   => Precision_cal=0.6825, Recall_cal=0.1432, AUC_cal=0.6395\n   => F1_cal=0.2367\n\nShape of feature matrix: (2225939, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6166, Recall=0.5491, AUC=0.6316\nF1=0.5809\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225939, 31)\n   => Precision_cal=0.6940, Recall_cal=0.2002, AUC_cal=0.6342\n   => F1_cal=0.3107\n\n\nTime-Series CV Results:\nAvg Precision: 0.6127\nAvg Recall:    0.5122\nAvg AUC:       0.6355\nAvg F1:        0.5573\nCalibrated:\nAvg Precision_cal: 0.6882\nAvg Recall_cal:    0.1717\nAvg AUC_cal:       0.6369\nAvg F1_cal:        0.2737\n[GridSearch] Param Set 14/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6355, Precision=0.6127, Recall=0.5122, f1=0.5573, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 25, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6369, Precision=0.6882, Recall=0.1717, f1=0.2737\nShape of feature matrix: (1696921, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6262, Recall=0.3849, AUC=0.6391\nF1=0.4767\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696921, 31)\n   => Precision_cal=0.6997, Recall_cal=0.0913, AUC_cal=0.6397\n   => F1_cal=0.1615\n\nShape of feature matrix: (2226234, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6070, Recall=0.5914, AUC=0.6295\nF1=0.5991\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226234, 31)\n   => Precision_cal=0.6825, Recall_cal=0.2670, AUC_cal=0.6343\n   => F1_cal=0.3838\n\n\nTime-Series CV Results:\nAvg Precision: 0.6166\nAvg Recall:    0.4881\nAvg AUC:       0.6343\nAvg F1:        0.5379\nCalibrated:\nAvg Precision_cal: 0.6911\nAvg Recall_cal:    0.1791\nAvg AUC_cal:       0.6370\nAvg F1_cal:        0.2727\n[GridSearch] Param Set 15/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6343, Precision=0.6166, Recall=0.4881, f1=0.5379, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6370, Precision=0.6911, Recall=0.1791, f1=0.2727\nShape of feature matrix: (1696873, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6092, Recall=0.4836, AUC=0.6400\nF1=0.5392\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696873, 31)\n   => Precision_cal=0.6898, Recall_cal=0.1436, AUC_cal=0.6416\n   => F1_cal=0.2377\n\nShape of feature matrix: (2225542, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6005, Recall=0.6249, AUC=0.6311\nF1=0.6125\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225542, 31)\n   => Precision_cal=0.6710, Recall_cal=0.2962, AUC_cal=0.6340\n   => F1_cal=0.4110\n\n\nTime-Series CV Results:\nAvg Precision: 0.6048\nAvg Recall:    0.5542\nAvg AUC:       0.6355\nAvg F1:        0.5758\nCalibrated:\nAvg Precision_cal: 0.6804\nAvg Recall_cal:    0.2199\nAvg AUC_cal:       0.6378\nAvg F1_cal:        0.3244\n[GridSearch] Param Set 16/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6355, Precision=0.6048, Recall=0.5542, f1=0.5758, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6378, Precision=0.6804, Recall=0.2199, f1=0.3244\nShape of feature matrix: (1696861, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6088, Recall=0.5444, AUC=0.6523\nF1=0.5748\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696861, 31)\n   => Precision_cal=0.6904, Recall_cal=0.2215, AUC_cal=0.6546\n   => F1_cal=0.3354\n\nShape of feature matrix: (2225953, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6067, Recall=0.6094, AUC=0.6337\nF1=0.6080\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225953, 31)\n   => Precision_cal=0.6951, Recall_cal=0.2416, AUC_cal=0.6386\n   => F1_cal=0.3585\n\n\nTime-Series CV Results:\nAvg Precision: 0.6077\nAvg Recall:    0.5769\nAvg AUC:       0.6430\nAvg F1:        0.5914\nCalibrated:\nAvg Precision_cal: 0.6928\nAvg Recall_cal:    0.2315\nAvg AUC_cal:       0.6466\nAvg F1_cal:        0.3470\n[GridSearch] Param Set 17/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6430, Precision=0.6077, Recall=0.5769, f1=0.5914, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6466, Precision=0.6928, Recall=0.2315, f1=0.3470\nShape of feature matrix: (1696840, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5937, Recall=0.6121, AUC=0.6487\nF1=0.6028\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696840, 31)\n   => Precision_cal=0.6666, Recall_cal=0.3056, AUC_cal=0.6547\n   => F1_cal=0.4191\n\nShape of feature matrix: (2225940, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.5868, Recall=0.7017, AUC=0.6337\nF1=0.6391\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225940, 31)\n   => Precision_cal=0.6839, Recall_cal=0.2827, AUC_cal=0.6389\n   => F1_cal=0.4000\n\n\nTime-Series CV Results:\nAvg Precision: 0.5902\nAvg Recall:    0.6569\nAvg AUC:       0.6412\nAvg F1:        0.6209\nCalibrated:\nAvg Precision_cal: 0.6752\nAvg Recall_cal:    0.2941\nAvg AUC_cal:       0.6468\nAvg F1_cal:        0.4095\n[GridSearch] Param Set 18/18 Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05} => AUC=0.6412, Precision=0.5902, Recall=0.6569, f1=0.6209, Params={Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n\nCalibrated: AUC=0.6468, Precision=0.6752, Recall=0.2941, f1=0.4095\n[GridSearch] Best Params: {Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}, AUC=-inf, Precision=0.5761, Recall=0.7055\n"
     ]
    }
   ],
   "source": [
    "# Call grid search function\n",
    "best_params, best_metrics, best_params_calibrated, best_metrics_calibrated = time_cv_years_with_gridsearch(\n",
    "    train_df=train_df,\n",
    "    pipeline=mlp_pipeline,\n",
    "    paramGrid=mlp_paramGrid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d1f6c0-00db-4fcf-b56d-3d6782fec097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n************\n(0.5760645418380439, 0.7054510213460385, 0.6392966767463464, 0.6320791488079622)\n************\n{Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_e56fbf6947b6', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.05}\n************\n(0.6537298353119751, 0.3817598069387791, 0.6435281527449627, 0.4653282473815495)\n************\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "print(\"************\")\n",
    "print(best_metrics)\n",
    "print(\"************\")\n",
    "print(best_params_calibrated)\n",
    "print(\"************\")\n",
    "print(best_metrics_calibrated)\n",
    "print(\"************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48077ea-87d5-47d8-8daa-70450afc6213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CV Results:\n",
    "{Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [32, 10, 2], Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n",
    "************\n",
    "Non Calibrated:\n",
    "(precision, recall, auc, f1)\n",
    "\n",
    "(0.8921949635310785, 0.768082360762984, 0.8868068885366374, 0.8242354873600939)\n",
    "************\n",
    "{Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [32, 10, 2], Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n",
    "************\n",
    "Calibrated:\n",
    "- Precision: 0.9368102975566611\n",
    "- Recall: 0.7069352376698033\n",
    "- AUC: 0.8865981546775914\n",
    "- F1: 0.8051884869202455\n",
    "\n",
    "(precision, recall, auc, f1)\n",
    "(0.9368102975566611, 0.7069352376698033, 0.8865981546775914, 0.8051884869202455)\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8ad716-20f7-4962-b45e-586300994d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: layers=[31, 10, 2], maxIter=100, stepSize=0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the final MLP with the best parameters\n",
    "best_layers = best_params_calibrated.get('layers', layer_configs[0])\n",
    "best_maxIter = best_params_calibrated.get('maxIter', 100)\n",
    "best_stepSize = best_params_calibrated.get('stepSize', 0.01)\n",
    "\n",
    "print(f\"Best parameters: layers={best_layers}, maxIter={best_maxIter}, stepSize={best_stepSize}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623b2ac4-9758-4c0a-b4bb-a5a9e0e8c104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Best Param:\n",
    "Best parameters: layers=[31, 10, 2], maxIter=100, stepSize=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13c8d2a-c0b1-4294-9d77-86b721ca292b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the final MLP model with best parameters\n",
    "final_mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    layers=best_layers,\n",
    "    maxIter=best_maxIter,\n",
    "    stepSize=best_stepSize,\n",
    "    solver=\"l-bfgs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54ab8c6-1907-4105-ab33-bfa95585ac2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Calibrated:\n=> Precision=0.2476, Recall=0.6788, AUC=0.6365\nF1=0.3629\nCalibrated:\n   => Precision_cal=0.3266, Recall_cal=0.2879, AUC_cal=0.6386\n   => F1_cal=0.3060\n\nTest metrics: Precision=0.24762758683551225, Recall=0.6787672106344826, AUC=0.6365222999903057, F1=0.3628722588933425\nCalibrated: Precision=0.3265543894866837, Recall=0.2878937592771078, AUC=0.6386191837076368, F1=0.30600782502773244\n"
     ]
    }
   ],
   "source": [
    "# Create the final pipeline\n",
    "final_mlp_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler, final_mlp])\n",
    "\n",
    "# Fit the final model on training data\n",
    "final_model = final_mlp_pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate on test data\n",
    "precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal = evaluate_ml_model(final_model, test_df)\n",
    "print(f\"Test metrics: Precision={precision}, Recall={recall}, AUC={auc}, F1={f1}\")\n",
    "print(f\"Calibrated: Precision={precision_cal}, Recall={recall_cal}, AUC={auc_cal}, F1={f1_cal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3863f5b-93a3-41fb-ab81-e58ef1458ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Train Test results with best parm:\n",
    "\n",
    "#### Train Results\n",
    "Non-Calibrated:\n",
    "- Precision=0.2476\n",
    "- Recall=0.6788\n",
    "- AUC=0.6365\n",
    "- F1=0.3629\n",
    "\n",
    "Calibrated:\n",
    "- Precision_cal=0.3266,\n",
    "- Recall_cal=0.2879\n",
    "- AUC_cal=0.6386\n",
    "- F1_cal=0.3060\n",
    "\n",
    "#### Test Results\n",
    "\n",
    "Non Calibrated: \n",
    "- Precision=0.24762758683551225\n",
    "- Recall=0.6787672106344826\n",
    "- AUC=0.6365222999903057\n",
    "- F1=0.3628722588933425\n",
    "\n",
    "Calibrated: \n",
    "- Precision=0.3265543894866837\n",
    "- Recall=0.2878937592771078\n",
    "- AUC=0.6386191837076368\n",
    "- F1=0.30600782502773244"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83e6c2b-b4bb-4c56-8554-7a1807395cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c684b8-23c1-4c6b-8652-ad87df296774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features based on weight analysis:\nFeature: HourlyAltimeterSetting, Score: 10.9176\nFeature: CRS_DEP_TIME, Score: 10.4867\nFeature: HourlyRelativeHumidity, Score: 8.8356\nFeature: origin_airport_lon, Score: 8.5671\nFeature: CRS_ARR_TIME, Score: 7.1204\nFeature: origin_airport_lat, Score: 6.8495\nFeature: HourlyDewPointTemperature, Score: 6.8400\nFeature: MONTH_index, Score: 6.7503\nFeature: rolling_3mo_delay_count, Score: 6.7112\nFeature: OP_UNIQUE_CARRIER_index, Score: 5.9972\n"
     ]
    }
   ],
   "source": [
    "# Extract the final MLP model from the fitted pipeline\n",
    "final_mlp_model = final_model.stages[-1]\n",
    "\n",
    "# Get the weights vector from the MLP model\n",
    "weights_vector = final_mlp_model.weights\n",
    "\n",
    "# Convert the weights to a NumPy array for easier manipulation\n",
    "import numpy as np\n",
    "weights_array = np.array(weights_vector.toArray())\n",
    "\n",
    "# Determine sizes: input_size and hidden_size are obtained from best_layers.\n",
    "input_size = best_layers[0]\n",
    "hidden_size = best_layers[1]\n",
    "\n",
    "# Extract weights for the first layer and reshape into a matrix\n",
    "# The first layer contains input_size * hidden_size weights.\n",
    "first_layer_weights = weights_array[: input_size * hidden_size]\n",
    "first_layer_matrix = first_layer_weights.reshape((input_size, hidden_size))\n",
    "\n",
    "# Calculate the aggregated importance for each input feature by summing\n",
    "# the absolute weights across all neurons in the first hidden layer.\n",
    "aggregated_importance = np.sum(np.abs(first_layer_matrix), axis=1)\n",
    "\n",
    "# Sort features by aggregated importance in descending order\n",
    "sorted_indices = np.argsort(aggregated_importance)[::-1]\n",
    "\n",
    "# Retrieve feature names from the vector assembler\n",
    "feature_names = vector_assembler.getInputCols()\n",
    "\n",
    "# Get the top 10 feature indices and their corresponding names and scores\n",
    "top_10_indices = sorted_indices[:10]\n",
    "top_features = [(feature_names[i], aggregated_importance[i])\n",
    "                for i in top_10_indices]\n",
    "\n",
    "print(\"Top 10 features based on weight analysis:\")\n",
    "for feature, score in top_features:\n",
    "    print(f\"Feature: {feature}, Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef215077-f155-4e49-a8b0-2ec3485e8db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLP 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf454a92-26c7-4911-8f48-77b1906949d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614ad32a-79e1-44d6-a084-b29118206498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_2layers = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    solver=\"l-bfgs\",\n",
    "    layers=[vector_size, 20, 10, 2]  # Default values with 2 hidden layers (20 and 10 neurons)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baf33bd-272e-4a0a-b1b6-f866eb553118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the full pipeline\n",
    "mlp_2layer_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler, mlp_2layers])\n",
    "\n",
    "# Format: [input_size, first_hidden_layer, second_hidden_layer, output_layer]\n",
    "layer_configs_2hidden = [\n",
    "    [vector_size, 20, 10, 2],  # Medium-sized first layer, smaller second layer\n",
    "    [vector_size, 30, 15, 2],  # Larger first layer, medium second layer\n",
    "    [vector_size, 40, 20, 2]   # Even larger layers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb87962e-3e70-4fec-aec1-1a31ce7c5b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create parameter grid for 2-hidden-layer MLP\n",
    "mlp_2layer_paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(mlp_2layers.layers, layer_configs_2hidden)\n",
    "    .addGrid(mlp_2layers.maxIter, [50, 100])\n",
    "    .addGrid(mlp_2layers.stepSize, [0.01, 0.03])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615c4eed-d9e1-4d2d-96ca-fb30463f689c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696952, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6335, Recall=0.3232, AUC=0.6346\nF1=0.4280\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696952, 31)\n   => Precision_cal=0.7046, Recall_cal=0.0678, AUC_cal=0.6371\n   => F1_cal=0.1237\n\nShape of feature matrix: (2226085, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6126, Recall=0.5665, AUC=0.6351\nF1=0.5887\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226085, 31)\n   => Precision_cal=0.6982, Recall_cal=0.2011, AUC_cal=0.6356\n   => F1_cal=0.3123\n\n\nTime-Series CV Results:\nAvg Precision: 0.6230\nAvg Recall:    0.4449\nAvg AUC:       0.6348\nAvg F1:        0.5083\nCalibrated:\nAvg Precision_cal: 0.7014\nAvg Recall_cal:    0.1345\nAvg AUC_cal:       0.6364\nAvg F1_cal:        0.2180\n[GridSearch] Param Set 1/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6348, Precision=0.6230, Recall=0.4449, f1=0.5083, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6364, Precision=0.7014, Recall=0.1345, f1=0.2180\nShape of feature matrix: (1696911, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6098, Recall=0.4874, AUC=0.6441\nF1=0.5418\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696911, 31)\n   => Precision_cal=0.6955, Recall_cal=0.1781, AUC_cal=0.6443\n   => F1_cal=0.2836\n\nShape of feature matrix: (2226176, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6160, Recall=0.5588, AUC=0.6384\nF1=0.5860\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226176, 31)\n   => Precision_cal=0.6964, Recall_cal=0.2374, AUC_cal=0.6381\n   => F1_cal=0.3541\n\n\nTime-Series CV Results:\nAvg Precision: 0.6129\nAvg Recall:    0.5231\nAvg AUC:       0.6412\nAvg F1:        0.5639\nCalibrated:\nAvg Precision_cal: 0.6959\nAvg Recall_cal:    0.2077\nAvg AUC_cal:       0.6412\nAvg F1_cal:        0.3188\n[GridSearch] Param Set 2/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03} => AUC=0.6412, Precision=0.6129, Recall=0.5231, f1=0.5639, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n\nCalibrated: AUC=0.6412, Precision=0.6959, Recall=0.2077, f1=0.3188\nShape of feature matrix: (1696992, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5826, Recall=0.6313, AUC=0.6494\nF1=0.6060\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696992, 31)\n   => Precision_cal=0.6462, Recall_cal=0.3796, AUC_cal=0.6500\n   => F1_cal=0.4783\n\nShape of feature matrix: (2226022, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6095, Recall=0.5787, AUC=0.6354\nF1=0.5937\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226022, 31)\n   => Precision_cal=0.6822, Recall_cal=0.2755, AUC_cal=0.6345\n   => F1_cal=0.3925\n\n\nTime-Series CV Results:\nAvg Precision: 0.5960\nAvg Recall:    0.6050\nAvg AUC:       0.6424\nAvg F1:        0.5998\nCalibrated:\nAvg Precision_cal: 0.6642\nAvg Recall_cal:    0.3275\nAvg AUC_cal:       0.6422\nAvg F1_cal:        0.4354\n[GridSearch] Param Set 3/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6424, Precision=0.5960, Recall=0.6050, f1=0.5998, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6422, Precision=0.6642, Recall=0.3275, f1=0.4354\nShape of feature matrix: (1696698, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5777, Recall=0.6510, AUC=0.6458\nF1=0.6121\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696698, 31)\n   => Precision_cal=0.6429, Recall_cal=0.3817, AUC_cal=0.6469\n   => F1_cal=0.4790\n\nShape of feature matrix: (2225977, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6067, Recall=0.6151, AUC=0.6413\nF1=0.6109\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225977, 31)\n   => Precision_cal=0.6922, Recall_cal=0.2735, AUC_cal=0.6411\n   => F1_cal=0.3921\n\n\nTime-Series CV Results:\nAvg Precision: 0.5922\nAvg Recall:    0.6330\nAvg AUC:       0.6435\nAvg F1:        0.6115\nCalibrated:\nAvg Precision_cal: 0.6675\nAvg Recall_cal:    0.3276\nAvg AUC_cal:       0.6440\nAvg F1_cal:        0.4355\n[GridSearch] Param Set 4/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03} => AUC=0.6435, Precision=0.5922, Recall=0.6330, f1=0.6115, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n\nCalibrated: AUC=0.6440, Precision=0.6675, Recall=0.3276, f1=0.4355\nShape of feature matrix: (1696952, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.4792, Recall=0.4926, AUC=0.4757\nF1=0.4858\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696952, 31)\n   => Precision_cal=0.0000, Recall_cal=0.0000, AUC_cal=0.4990\n   => F1_cal=0.0000\n\nShape of feature matrix: (2225912, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6139, Recall=0.5630, AUC=0.6333\nF1=0.5874\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225912, 31)\n   => Precision_cal=0.6907, Recall_cal=0.2123, AUC_cal=0.6345\n   => F1_cal=0.3248\n\n\nTime-Series CV Results:\nAvg Precision: 0.5465\nAvg Recall:    0.5278\nAvg AUC:       0.5545\nAvg F1:        0.5366\nCalibrated:\nAvg Precision_cal: 0.3454\nAvg Recall_cal:    0.1062\nAvg AUC_cal:       0.5667\nAvg F1_cal:        0.1624\n[GridSearch] Param Set 5/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.5545, Precision=0.5465, Recall=0.5278, f1=0.5366, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.5667, Precision=0.3454, Recall=0.1062, f1=0.1624\nShape of feature matrix: (1696757, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.4793, Recall=0.4945, AUC=0.4757\nF1=0.4868\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696757, 31)\n   => Precision_cal=0.0000, Recall_cal=0.0000, AUC_cal=0.4989\n   => F1_cal=0.0000\n\nShape of feature matrix: (2225938, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6150, Recall=0.5553, AUC=0.6349\nF1=0.5836\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225938, 31)\n   => Precision_cal=0.6985, Recall_cal=0.1985, AUC_cal=0.6359\n   => F1_cal=0.3091\n\n\nTime-Series CV Results:\nAvg Precision: 0.5472\nAvg Recall:    0.5249\nAvg AUC:       0.5553\nAvg F1:        0.5352\nCalibrated:\nAvg Precision_cal: 0.3492\nAvg Recall_cal:    0.0993\nAvg AUC_cal:       0.5674\nAvg F1_cal:        0.1546\n[GridSearch] Param Set 6/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03} => AUC=0.5553, Precision=0.5472, Recall=0.5249, f1=0.5352, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n\nCalibrated: AUC=0.5674, Precision=0.3492, Recall=0.0993, f1=0.1546\nShape of feature matrix: (1696994, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.4767, Recall=0.6182, AUC=0.4755\nF1=0.5383\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696994, 31)\n   => Precision_cal=0.0000, Recall_cal=0.0000, AUC_cal=0.4989\n   => F1_cal=0.0000\n\nShape of feature matrix: (2225916, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.5993, Recall=0.6395, AUC=0.6359\nF1=0.6187\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225916, 31)\n   => Precision_cal=0.6883, Recall_cal=0.2829, AUC_cal=0.6389\n   => F1_cal=0.4010\n\n\nTime-Series CV Results:\nAvg Precision: 0.5380\nAvg Recall:    0.6288\nAvg AUC:       0.5557\nAvg F1:        0.5785\nCalibrated:\nAvg Precision_cal: 0.3441\nAvg Recall_cal:    0.1415\nAvg AUC_cal:       0.5689\nAvg F1_cal:        0.2005\n[GridSearch] Param Set 7/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.5557, Precision=0.5380, Recall=0.6288, f1=0.5785, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.5689, Precision=0.3441, Recall=0.1415, f1=0.2005\nShape of feature matrix: (1697086, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.4765, Recall=0.6264, AUC=0.4755\nF1=0.5413\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1697086, 31)\n   => Precision_cal=0.0000, Recall_cal=0.0000, AUC_cal=0.4989\n   => F1_cal=0.0000\n\nShape of feature matrix: (2226096, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.5977, Recall=0.6317, AUC=0.6335\nF1=0.6142\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2226096, 31)\n   => Precision_cal=0.6752, Recall_cal=0.3007, AUC_cal=0.6344\n   => F1_cal=0.4161\n\n\nTime-Series CV Results:\nAvg Precision: 0.5371\nAvg Recall:    0.6290\nAvg AUC:       0.5545\nAvg F1:        0.5778\nCalibrated:\nAvg Precision_cal: 0.3376\nAvg Recall_cal:    0.1504\nAvg AUC_cal:       0.5667\nAvg F1_cal:        0.2081\n[GridSearch] Param Set 8/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03} => AUC=0.5545, Precision=0.5371, Recall=0.6290, f1=0.5778, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 30, 15, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n\nCalibrated: AUC=0.5667, Precision=0.3376, Recall=0.1504, f1=0.2081\nShape of feature matrix: (1696865, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5974, Recall=0.5271, AUC=0.6416\nF1=0.5601\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696865, 31)\n   => Precision_cal=0.6813, Recall_cal=0.1488, AUC_cal=0.6399\n   => F1_cal=0.2442\n\nShape of feature matrix: (2225946, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6210, Recall=0.5246, AUC=0.6336\nF1=0.5688\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225946, 31)\n   => Precision_cal=0.7047, Recall_cal=0.1794, AUC_cal=0.6350\n   => F1_cal=0.2861\n\n\nTime-Series CV Results:\nAvg Precision: 0.6092\nAvg Recall:    0.5259\nAvg AUC:       0.6376\nAvg F1:        0.5644\nCalibrated:\nAvg Precision_cal: 0.6930\nAvg Recall_cal:    0.1641\nAvg AUC_cal:       0.6375\nAvg F1_cal:        0.2651\n[GridSearch] Param Set 9/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6376, Precision=0.6092, Recall=0.5259, f1=0.5644, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6375, Precision=0.6930, Recall=0.1641, f1=0.2651\nShape of feature matrix: (1696941, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5979, Recall=0.5339, AUC=0.6429\nF1=0.5641\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696941, 31)\n   => Precision_cal=0.6782, Recall_cal=0.1745, AUC_cal=0.6413\n   => F1_cal=0.2775\n\nShape of feature matrix: (2225995, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6117, Recall=0.5683, AUC=0.6329\nF1=0.5892\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225995, 31)\n   => Precision_cal=0.6876, Recall_cal=0.2173, AUC_cal=0.6341\n   => F1_cal=0.3303\n\n\nTime-Series CV Results:\nAvg Precision: 0.6048\nAvg Recall:    0.5511\nAvg AUC:       0.6379\nAvg F1:        0.5766\nCalibrated:\nAvg Precision_cal: 0.6829\nAvg Recall_cal:    0.1959\nAvg AUC_cal:       0.6377\nAvg F1_cal:        0.3039\n[GridSearch] Param Set 10/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03} => AUC=0.6379, Precision=0.6048, Recall=0.5511, f1=0.5766, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n\nCalibrated: AUC=0.6377, Precision=0.6829, Recall=0.1959, f1=0.3039\nShape of feature matrix: (1696731, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5871, Recall=0.6070, AUC=0.6496\nF1=0.5969\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696731, 31)\n   => Precision_cal=0.6831, Recall_cal=0.2346, AUC_cal=0.6486\n   => F1_cal=0.3493\n\nShape of feature matrix: (2225887, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.5847, Recall=0.6926, AUC=0.6318\nF1=0.6341\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225887, 31)\n   => Precision_cal=0.6587, Recall_cal=0.3500, AUC_cal=0.6333\n   => F1_cal=0.4571\n\n\nTime-Series CV Results:\nAvg Precision: 0.5859\nAvg Recall:    0.6498\nAvg AUC:       0.6407\nAvg F1:        0.6155\nCalibrated:\nAvg Precision_cal: 0.6709\nAvg Recall_cal:    0.2923\nAvg AUC_cal:       0.6410\nAvg F1_cal:        0.4032\n[GridSearch] Param Set 11/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01} => AUC=0.6407, Precision=0.5859, Recall=0.6498, f1=0.6155, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n\nCalibrated: AUC=0.6410, Precision=0.6709, Recall=0.2923, f1=0.4032\nShape of feature matrix: (1696845, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.6008, Recall=0.5313, AUC=0.6481\nF1=0.5639\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696845, 31)\n   => Precision_cal=0.6974, Recall_cal=0.1781, AUC_cal=0.6469\n   => F1_cal=0.2838\n\nShape of feature matrix: (2225884, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6157, Recall=0.5560, AUC=0.6267\nF1=0.5844\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225884, 31)\n   => Precision_cal=0.7132, Recall_cal=0.1890, AUC_cal=0.6361\n   => F1_cal=0.2988\n\n\nTime-Series CV Results:\nAvg Precision: 0.6083\nAvg Recall:    0.5437\nAvg AUC:       0.6374\nAvg F1:        0.5741\nCalibrated:\nAvg Precision_cal: 0.7053\nAvg Recall_cal:    0.1836\nAvg AUC_cal:       0.6415\nAvg F1_cal:        0.2913\n[GridSearch] Param Set 12/12 Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03} => AUC=0.6374, Precision=0.6083, Recall=0.5437, f1=0.5741, Params={Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n\nCalibrated: AUC=0.6415, Precision=0.7053, Recall=0.1836, f1=0.2913\n[GridSearch] Best Params: {Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}, AUC=-inf, Precision=0.5859, Recall=0.6498\nBest parameters for 2-layer MLP: layers=[31, 30, 15, 2], maxIter=100, stepSize=0.01\n"
     ]
    }
   ],
   "source": [
    "best_params_2layer, best_metrics_2layer, best_params_calibrated_2layer, best_metrics_calibrated_2layer = time_cv_years_with_gridsearch(\n",
    "    train_df=train_df,\n",
    "    pipeline=mlp_2layer_pipeline,\n",
    "    paramGrid=mlp_2layer_paramGrid\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Get the best parameters\n",
    "best_layers_2layer = best_params_calibrated_2layer.get('layers', [vector_size, 30, 15, 2])\n",
    "best_maxIter_2layer = best_params_calibrated_2layer.get('maxIter', 100)\n",
    "best_stepSize_2layer = best_params_calibrated_2layer.get('stepSize', 0.01)\n",
    "\n",
    "print(f\"Best parameters for 2-layer MLP: layers={best_layers_2layer}, maxIter={best_maxIter_2layer}, stepSize={best_stepSize_2layer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca72b830-d233-4333-a2d9-5052e7f0e8c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 40, 20, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n************\n(0.5858998787299918, 0.6498049637588736, 0.6407055349265882, 0.6154909950307311)\n************\n{Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 20, 10, 2], Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_0cb20715c5de', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.03}\n************\n(0.6675165460349738, 0.32759106720009556, 0.6439996301953947, 0.4355339250416386)\n************\n"
     ]
    }
   ],
   "source": [
    "print(best_params_2layer)\n",
    "print(\"************\")\n",
    "print(best_metrics_2layer)\n",
    "print(\"************\")\n",
    "print(best_params_calibrated_2layer)\n",
    "print(\"************\")\n",
    "print(best_metrics_calibrated_2layer)\n",
    "print(\"************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dae128c-4174-4862-9e81-443bd9bef984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='MultilayerPerceptronClassifier_899632fbcbe5', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_899632fbcbe5', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_899632fbcbe5', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n************\n(0.27267268560795327, 0.5491666110004051, 0.6420380220541615, 0.36440879691606537)\n************\n{Param(parent='MultilayerPerceptronClassifier_899632fbcbe5', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [31, 10, 2], Param(parent='MultilayerPerceptronClassifier_899632fbcbe5', name='maxIter', doc='max number of iterations (>= 0).'): 50, Param(parent='MultilayerPerceptronClassifier_899632fbcbe5', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n************\n(0.3644486745529651, 0.17971411543129495, 0.643865332595127, 0.24072418170778825)\n************\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "print(\"************\")\n",
    "print(best_metrics)\n",
    "print(\"************\")\n",
    "print(best_params_calibrated)\n",
    "print(\"************\")\n",
    "print(best_metrics_calibrated)\n",
    "print(\"************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7bfd67-daee-4057-9a18-95f4ec04d274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###CV Results\n",
    "{Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [32, 10, 2], Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n",
    "************\n",
    "Non Calibrated:\n",
    "(precision, recall, auc, f1)\n",
    "\n",
    "(0.8921949635310785, 0.768082360762984, 0.8868068885366374, 0.8242354873600939)\n",
    "************\n",
    "{Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [32, 10, 2], Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_72eef61641c1', name='stepSize', doc='Step size to be used for each iteration of optimization (>= 0).'): 0.01}\n",
    "************\n",
    "Calibrated:\n",
    "- Precision: 0.9368102975566611\n",
    "- Recall: 0.7069352376698033\n",
    "- AUC: 0.8865981546775914\n",
    "- F1: 0.8051884869202455\n",
    "\n",
    "(precision, recall, auc, f1)\n",
    "\n",
    "(0.9368102975566611, 0.7069352376698033, 0.8865981546775914, 0.8051884869202455)\n",
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3069e8f9-3c06-415d-a834-2003ecdaf794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Best Param:\n",
    "layers=[32, 30, 15, 2], maxIter=100, stepSize=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bece21ca-a829-4546-8af4-c85054760871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the final 2-layer MLP model with best parameters\n",
    "final_mlp_2layer = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    layers=best_layers_2layer,\n",
    "    maxIter=best_maxIter_2layer,\n",
    "    stepSize=best_stepSize_2layer,\n",
    "    solver=\"l-bfgs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a28c463-06dd-4fa3-b2ea-ea8603ed8a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Calibrated:\n=> Precision=0.2653, Recall=0.5855, AUC=0.6399\nF1=0.3652\nCalibrated:\n   => Precision_cal=0.3489, Recall_cal=0.2330, AUC_cal=0.6420\n   => F1_cal=0.2794\n\nPrecision: 0.2653317575791887, Recall: 0.5854786532176979, AUC: 0.6399170502337804, F1: 0.3651720244886232\nCalibrated: Precision: 0.34889999246481196, Recall: 0.23297935374506704, AUC: 0.6420499272056805, F1: 0.2793929542114777\n"
     ]
    }
   ],
   "source": [
    "# Create the final pipeline\n",
    "final_mlp_2layer_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler, final_mlp_2layer])\n",
    "\n",
    "# Fit the final model on training data\n",
    "final_model_2layer = final_mlp_2layer_pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate on test data\n",
    "precision_2l, recall_2l, auc_2l, f1_2l, precision_cal_2l, recall_cal_2l, auc_cal_2l, f1_cal_2l = evaluate_ml_model(final_model_2layer, test_df)\n",
    "print(f\"Precision: {precision_2l}, Recall: {recall_2l}, AUC: {auc_2l}, F1: {f1_2l}\")\n",
    "print(f\"Calibrated: Precision: {precision_cal_2l}, Recall: {recall_cal_2l}, AUC: {auc_cal_2l}, F1: {f1_cal_2l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c03cf4-bfd5-4d21-b403-ede24694f557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Train Test Results with best params:\n",
    "\n",
    "Non-Calibrated:\n",
    "=> Precision=0.6481, Recall=0.6838, AUC=0.8634\n",
    "F1=0.6655\n",
    "Calibrated:\n",
    "   => Precision_cal=0.7408, Recall_cal=0.6276, AUC_cal=0.8633\n",
    "   => F1_cal=0.6795\n",
    "\n",
    "#### Test Results\n",
    "\n",
    "Non-Calibrated:\n",
    "- Precision: 0.2653317575791887\n",
    "- Recall: 0.5854786532176979\n",
    "- AUC: 0.6399170502337804\n",
    "- F1: 0.3651720244886232\n",
    "\n",
    "Calibrated: \n",
    "- Precision: 0.34889999246481196\n",
    "- Recall: 0.23297935374506704\n",
    "- AUC: 0.6420499272056805\n",
    "- F1: 0.2793929542114777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bef3e2-7aac-4e3f-bcda-12efdd5e16f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forest Model\n",
    "Umesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06253a9c-d434-45dc-b6d8-e33df8574dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a78ccf-e048-44d7-8527-40aaf658a94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "categorical_cols = [\n",
    "  \"OP_UNIQUE_CARRIER\", \"origin_type\", \"dest_type\",\n",
    "    \"QUARTER\", \"IS_HOLIDAY\", \"YEAR\", \"MONTH\", \"HAS_WEATHER_TYPE\", \"DAY_OF_WEEK\"\n",
    "]\n",
    "\n",
    "# Numerical columns (removed the duplicate \"HourlyPrecipitation\")\n",
    "numerical_cols = [\n",
    "    \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"origin_airport_lat\", \"origin_airport_lon\", \n",
    "    \"dest_airport_lat\", \"dest_airport_lon\", \"sched_depart_unix\", \"ELEVATION\",\n",
    "    \"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyPrecipitation\",\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyRelativeHumidity\", \"HourlyStationPressure\",\n",
    "    \"HourlyVisibility\", \"HourlyWetBulbTemperature\", \"HourlyWindDirection\",\n",
    "    \"HourlyWindSpeed\", \"rolling_3mo_delay_count\", \"DAY_OF_MONTH\", \n",
    "]\n",
    "\n",
    "\n",
    "# 1) StringIndex each categorical column\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "\n",
    "# 3) VectorAssembler for all features (OHE + numeric)\n",
    "feature_cols = [f\"{col}_index\" for col in categorical_cols] + numerical_cols\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"assembled_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 4) MinMaxScaler on assembled_features -> scaled_features\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"assembled_features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c1479f-0acd-4157-9a87-2eb943937bbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier   \n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    predictionCol=\"prediction\",\n",
    "\tprobabilityCol=\"probability\",\n",
    "\tnumTrees=10\n",
    ")\n",
    "# Create a pipeline\n",
    "random_forest_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler, rf])\n",
    "\n",
    "# # Fit the pipeline to the training data\n",
    "# model = random_forest_pipeline.fit(train_df)\n",
    "\n",
    "# # Make predictions on test data\n",
    "# predictions = model.transform(test_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# evaluator = BinaryClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2d3751-5882-4537-84e0-8ca59d7a8c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tailor it for Random Forest\n",
    "rf_paramGrid = (\n",
    "\n",
    "ParamGridBuilder()\n",
    ".addGrid(rf.numTrees, [10, 15, 20]) \\\n",
    ".addGrid(rf.maxDepth, [5, 8, 12]) \\\n",
    ".build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433acb26-1da5-4730-b71c-7ab362f9c49c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27773e7cc8f4c74b9d5d41b6cebfa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7aec611b7e494c913895bcf612dd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696321, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5749, Recall=0.6297, AUC=0.6370\nF1=0.6010\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696321, 31)\n   => Precision_cal=0.7325, Recall_cal=0.0744, AUC_cal=0.6370\n   => F1_cal=0.1351\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433424538c53495e9caac20f53126596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854340b53c214705911652b35af0d806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2224963, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6071, Recall=0.6276, AUC=0.6457\nF1=0.6172\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2224963, 31)\n   => Precision_cal=0.7794, Recall_cal=0.0841, AUC_cal=0.6457\n   => F1_cal=0.1519\n\n\nTime-Series CV Results:\nAvg Precision: 0.5910\nAvg Recall:    0.6287\nAvg AUC:       0.6414\nAvg F1:        0.6091\nCalibrated:\nAvg Precision_cal: 0.7560\nAvg Recall_cal:    0.0793\nAvg AUC_cal:       0.6414\nAvg F1_cal:        0.1435\n[GridSearch] Param Set 1/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5} => AUC=0.6414, Precision=0.5910, Recall=0.6287, f1=0.6091, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}\n\nCalibrated: AUC=0.6414, Precision=0.7560, Recall=0.0793, f1=0.1435\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92324bfa101642f796154815db5d6f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038735c2e6b04f408e78267b4ebf339c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696226, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5794, Recall=0.6400, AUC=0.6447\nF1=0.6082\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696226, 31)\n   => Precision_cal=0.6917, Recall_cal=0.1626, AUC_cal=0.6447\n   => F1_cal=0.2633\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea9109127034297bd5ed913996edb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcdd62f288c40f9b613eb539b93fe0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2224970, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6245, Recall=0.5952, AUC=0.6555\nF1=0.6095\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2224970, 31)\n   => Precision_cal=0.7579, Recall_cal=0.1497, AUC_cal=0.6555\n   => F1_cal=0.2500\n\n\nTime-Series CV Results:\nAvg Precision: 0.6020\nAvg Recall:    0.6176\nAvg AUC:       0.6501\nAvg F1:        0.6089\nCalibrated:\nAvg Precision_cal: 0.7248\nAvg Recall_cal:    0.1561\nAvg AUC_cal:       0.6501\nAvg F1_cal:        0.2567\n[GridSearch] Param Set 2/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 8} => AUC=0.6501, Precision=0.6020, Recall=0.6176, f1=0.6089, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 8}\n\nCalibrated: AUC=0.6501, Precision=0.7248, Recall=0.1561, f1=0.2567\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997b464b9f0045b5b05d2b182f958ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fdbdd904a34ad8a22775ed287ee661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696400, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5770, Recall=0.6699, AUC=0.6479\nF1=0.6200\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696400, 31)\n   => Precision_cal=0.6884, Recall_cal=0.1642, AUC_cal=0.6479\n   => F1_cal=0.2652\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0c4c91083f4348ab2b6cf728029676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c4e6b17ae841e09b7156b805fcbb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2225011, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6337, Recall=0.5901, AUC=0.6650\nF1=0.6111\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2225011, 31)\n   => Precision_cal=0.7399, Recall_cal=0.2245, AUC_cal=0.6650\n   => F1_cal=0.3444\n\n\nTime-Series CV Results:\nAvg Precision: 0.6054\nAvg Recall:    0.6300\nAvg AUC:       0.6564\nAvg F1:        0.6156\nCalibrated:\nAvg Precision_cal: 0.7141\nAvg Recall_cal:    0.1943\nAvg AUC_cal:       0.6564\nAvg F1_cal:        0.3048\n[GridSearch] Param Set 3/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12} => AUC=0.6564, Precision=0.6054, Recall=0.6300, f1=0.6156, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n\nCalibrated: AUC=0.6564, Precision=0.7141, Recall=0.1943, f1=0.3048\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef22a3f167114c1085349c906304d72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcbb38a189f41a6a306113e71588db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696205, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5687, Recall=0.6857, AUC=0.6423\nF1=0.6217\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696205, 31)\n   => Precision_cal=0.7228, Recall_cal=0.0738, AUC_cal=0.6423\n   => F1_cal=0.1339\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b36c65169346708c41617049d95e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f069a8f23f6d456fb84655c06f396bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2224507, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6014, Recall=0.6323, AUC=0.6410\nF1=0.6165\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2224507, 31)\n   => Precision_cal=0.7863, Recall_cal=0.0660, AUC_cal=0.6410\n   => F1_cal=0.1218\n\n\nTime-Series CV Results:\nAvg Precision: 0.5850\nAvg Recall:    0.6590\nAvg AUC:       0.6416\nAvg F1:        0.6191\nCalibrated:\nAvg Precision_cal: 0.7545\nAvg Recall_cal:    0.0699\nAvg AUC_cal:       0.6416\nAvg F1_cal:        0.1279\n[GridSearch] Param Set 4/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5} => AUC=0.6416, Precision=0.5850, Recall=0.6590, f1=0.6191, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}\n\nCalibrated: AUC=0.6416, Precision=0.7545, Recall=0.0699, f1=0.1279\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0847472604d743e28ef7dc95756a572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9126b348307462590fd16eb9fe9ef44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696320, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5659, Recall=0.7244, AUC=0.6507\nF1=0.6354\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696320, 31)\n   => Precision_cal=0.7161, Recall_cal=0.1358, AUC_cal=0.6507\n   => F1_cal=0.2284\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1a629e6fb14fc0b9ab4730aa92b16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09b9f572cfe4201bfe1a47c09d308f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2222778, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6209, Recall=0.6039, AUC=0.6559\nF1=0.6123\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2222778, 31)\n   => Precision_cal=0.7771, Recall_cal=0.1093, AUC_cal=0.6559\n   => F1_cal=0.1916\n\n\nTime-Series CV Results:\nAvg Precision: 0.5934\nAvg Recall:    0.6641\nAvg AUC:       0.6533\nAvg F1:        0.6238\nCalibrated:\nAvg Precision_cal: 0.7466\nAvg Recall_cal:    0.1225\nAvg AUC_cal:       0.6533\nAvg F1_cal:        0.2100\n[GridSearch] Param Set 5/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 8} => AUC=0.6533, Precision=0.5934, Recall=0.6641, f1=0.6238, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 8}\n\nCalibrated: AUC=0.6533, Precision=0.7466, Recall=0.1225, f1=0.2100\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ee4846148443ce9235c47bc6c02a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8df50274b2b49a89f14806dc4578b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696210, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5573, Recall=0.7543, AUC=0.6459\nF1=0.6410\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696210, 31)\n   => Precision_cal=0.6507, Recall_cal=0.2818, AUC_cal=0.6459\n   => F1_cal=0.3933\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c508cb0d210749339578d5109a377be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf327a8a251400389f62f6ab2247a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2224394, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6386, Recall=0.5796, AUC=0.6667\nF1=0.6077\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2224394, 31)\n   => Precision_cal=0.7536, Recall_cal=0.2041, AUC_cal=0.6667\n   => F1_cal=0.3212\n\n\nTime-Series CV Results:\nAvg Precision: 0.5979\nAvg Recall:    0.6669\nAvg AUC:       0.6563\nAvg F1:        0.6243\nCalibrated:\nAvg Precision_cal: 0.7022\nAvg Recall_cal:    0.2429\nAvg AUC_cal:       0.6563\nAvg F1_cal:        0.3572\n[GridSearch] Param Set 6/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12} => AUC=0.6563, Precision=0.5979, Recall=0.6669, f1=0.6243, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n\nCalibrated: AUC=0.6563, Precision=0.7022, Recall=0.2429, f1=0.3572\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e18cef782c54800931464e76bf3fc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffba149b8dc4eb1919649dde763b0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696358, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5779, Recall=0.6330, AUC=0.6442\nF1=0.6042\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696358, 31)\n   => Precision_cal=0.7381, Recall_cal=0.0658, AUC_cal=0.6442\n   => F1_cal=0.1208\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a57fa9e814a429b9e52b81197c81ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61b88b87c514473b18bf8b2d096f3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2224557, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6033, Recall=0.6168, AUC=0.6434\nF1=0.6100\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2224557, 31)\n   => Precision_cal=0.7735, Recall_cal=0.0953, AUC_cal=0.6434\n   => F1_cal=0.1697\n\n\nTime-Series CV Results:\nAvg Precision: 0.5906\nAvg Recall:    0.6249\nAvg AUC:       0.6438\nAvg F1:        0.6071\nCalibrated:\nAvg Precision_cal: 0.7558\nAvg Recall_cal:    0.0805\nAvg AUC_cal:       0.6438\nAvg F1_cal:        0.1453\n[GridSearch] Param Set 7/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5} => AUC=0.6438, Precision=0.5906, Recall=0.6249, f1=0.6071, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}\n\nCalibrated: AUC=0.6438, Precision=0.7558, Recall=0.0805, f1=0.1453\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735361cceed049aeb70b4130db25149a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9789a71a067948e595de664b65e408f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696209, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5759, Recall=0.6776, AUC=0.6529\nF1=0.6226\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696209, 31)\n   => Precision_cal=0.7053, Recall_cal=0.1743, AUC_cal=0.6529\n   => F1_cal=0.2796\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2946fcbd12aa41d18faf90a12af273e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee9693bd3a840c5b8935ebd66dbb959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2224867, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6271, Recall=0.5936, AUC=0.6572\nF1=0.6099\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2224867, 31)\n   => Precision_cal=0.7541, Recall_cal=0.1625, AUC_cal=0.6572\n   => F1_cal=0.2674\n\n\nTime-Series CV Results:\nAvg Precision: 0.6015\nAvg Recall:    0.6356\nAvg AUC:       0.6551\nAvg F1:        0.6163\nCalibrated:\nAvg Precision_cal: 0.7297\nAvg Recall_cal:    0.1684\nAvg AUC_cal:       0.6551\nAvg F1_cal:        0.2735\n[GridSearch] Param Set 8/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 8} => AUC=0.6551, Precision=0.6015, Recall=0.6356, f1=0.6163, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 8}\n\nCalibrated: AUC=0.6551, Precision=0.7297, Recall=0.1684, f1=0.2735\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3b707cb42844a09d6c91485f60d53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deb52b575cf45c38342b0945a431218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1696247, 31)\nNon-Calibrated:\nFold: Train <= Year 2015, Validate = Year 2016 => Precision=0.5789, Recall=0.6833, AUC=0.6563\nF1=0.6268\nCalibrated:\nFold: Train <= 2015, Validate = 2016\n   => Shape: (1696247, 31)\n   => Precision_cal=0.6979, Recall_cal=0.2079, AUC_cal=0.6563\n   => F1_cal=0.3204\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35158a236f4e45a5b82bd15877a3e700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee35f19b06b4d97952c512096a57dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (2223856, 31)\nNon-Calibrated:\nFold: Train <= Year 2017, Validate = Year 2018 => Precision=0.6356, Recall=0.5948, AUC=0.6683\nF1=0.6145\nCalibrated:\nFold: Train <= 2017, Validate = 2018\n   => Shape: (2223856, 31)\n   => Precision_cal=0.7512, Recall_cal=0.2164, AUC_cal=0.6683\n   => F1_cal=0.3360\n\n\nTime-Series CV Results:\nAvg Precision: 0.6072\nAvg Recall:    0.6390\nAvg AUC:       0.6623\nAvg F1:        0.6207\nCalibrated:\nAvg Precision_cal: 0.7245\nAvg Recall_cal:    0.2121\nAvg AUC_cal:       0.6623\nAvg F1_cal:        0.3282\n[GridSearch] Param Set 9/9 Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12} => AUC=0.6623, Precision=0.6072, Recall=0.6390, f1=0.6207, Params={Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n\nCalibrated: AUC=0.6623, Precision=0.7245, Recall=0.2121, f1=0.3282\n[GridSearch] Best Params: {Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}, AUC=-inf, Precision=0.5979, Recall=0.6669\n"
     ]
    }
   ],
   "source": [
    "# Tailor it for Random Forest\n",
    "best_params, best_metrics, best_params_calibrated, best_metrics_calibrated = time_cv_years_with_gridsearch(\n",
    "train_df=train_df,\n",
    "pipeline=random_forest_pipeline, #pass the pipeline with yur model\n",
    "paramGrid=rf_paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3700d3a0-3766-4322-811b-028875e81864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n************\n(0.597948967274389, 0.6669227841282295, 0.6562678095825172, 0.6243239845704931)\n************\n{Param(parent='RandomForestClassifier_337537bc3d72', name='numTrees', doc='Number of trees to train (>= 1).'): 15, Param(parent='RandomForestClassifier_337537bc3d72', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n************\n(0.7021529669023652, 0.24294670787691763, 0.6562682306156873, 0.3572393823242457)\n************\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n",
    "print(\"************\")\n",
    "print(best_metrics)\n",
    "print(\"************\")\n",
    "print(best_params_calibrated)\n",
    "print(\"************\")\n",
    "print(best_metrics_calibrated)\n",
    "print(\"************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a2ef1d-4672-461f-99e6-c4f2f9cdef49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###CV Results\n",
    "\n",
    "{Param(parent='RandomForestClassifier_1427ed233d95', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_1427ed233d95', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n",
    "************\n",
    "Non Calibrated:\n",
    "(precision, recall, auc, f1)\n",
    "\n",
    "\n",
    "(0.6985803483739265, 0.7382673746996642, 0.7884390604247213, 0.7149955312522794)\n",
    "************\n",
    "{Param(parent='RandomForestClassifier_1427ed233d95', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_1427ed233d95', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 12}\n",
    "************\n",
    "Calibrated:\n",
    "- Precision: 0.8634837406271338\n",
    "- Recall: 0.3979052541561583\n",
    "- AUC: 0.7884397820446747\n",
    "- F1: 0.5441362696717309\n",
    "\n",
    "(precision, recall, auc, f1)\n",
    "\n",
    "\n",
    "(0.8634837406271338, 0.3979052541561583, 0.7884397820446747, 0.5441362696717309)\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cb23c0-3a3f-470c-aed7-a15064cb70e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_numTrees: 20\nbest_maxDepth: 12\n"
     ]
    }
   ],
   "source": [
    "#get best params\n",
    "best_numTrees = best_params_calibrated.get(\"numTrees\", 20)\n",
    "best_maxDepth = best_params_calibrated.get(\"maxDepth\", 12)\n",
    "\n",
    "print(f\"best_numTrees: {best_numTrees}\")\n",
    "print(f\"best_maxDepth: {best_maxDepth}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df17d9bb-fa81-4f89-b20e-f3a7b5555c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Best Params:\n",
    "best_numTrees: 20\n",
    "\n",
    "best_maxDepth: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f350f9-f7c5-4f17-bd73-eff069152a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\nWARNING:urllib3.connectionpool:Connection pool is full, discarding connection: oregon.cloud.databricks.com. Connection pool size: 10\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eac59592324532868195a86c586d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67b11ac0aeb441ba5da596bf1258c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we have the best parameters, let's train the model with the best parameters (Calibrated)\n",
    "# Create a Random Forest Classifier   \n",
    "rf_calibrated  = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"DELAY_TARGET\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    predictionCol=\"prediction\",\n",
    "\tprobabilityCol=\"probability\",\n",
    "\tnumTrees=best_numTrees,\n",
    "    maxDepth=best_maxDepth\n",
    ")\n",
    "# Create a pipeline\n",
    "random_forest_pipeline = Pipeline(stages=indexers + [vector_assembler, scaler, rf])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_calibrated   = random_forest_pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15974aba-1796-4dea-a206-360b010a9feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Calibrated:\n=> Precision=0.2628, Recall=0.6380, AUC=0.6485\nF1=0.3722\nCalibrated:\n   => Precision_cal=0.4545, Recall_cal=0.0970, AUC_cal=0.6485\n   => F1_cal=0.1599\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.2627831305841777,\n",
       " 0.6379848061469433,\n",
       " 0.648503943616904,\n",
       " 0.3722415902875936,\n",
       " 0.45452771669098846,\n",
       " 0.09700417193327336,\n",
       " 0.648506615716258,\n",
       " 0.1598858948602641)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate on test\n",
    "\n",
    "precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal = evaluate_ml_model(model_calibrated, test_df)\n",
    "precision, recall, auc, f1, precision_cal, recall_cal, auc_cal, f1_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2782e7af-672e-4767-88a2-6affc0a0d78c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Best Param Model train test results:\n",
    "\n",
    "Non-Calibrated:\n",
    "- Precision=0.2628\n",
    "- Recall=0.6380\n",
    "- AUC=0.6485\n",
    "- F1=0.3722\n",
    "\n",
    "Calibrated:\n",
    "- Precision_cal=0.4545\n",
    "- Recall_cal=0.0970\n",
    "- AUC_cal=0.6485\n",
    "- F1_cal=0.1599\n",
    "\n",
    "(0.2709525407708272,\n",
    " 0.6863658722151326,\n",
    " 0.6875188157235367,\n",
    " 0.3885281520805818,\n",
    " 0.5237647141749489,\n",
    " 0.12892566428646407,\n",
    " 0.6875057139402567,\n",
    " 0.20691806079322325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79662715-a818-423c-bfa1-f4830a150fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract pipeline stages\n",
    "assembler = [s for s in model_calibrated.stages if isinstance(s, VectorAssembler)][0]\n",
    "ohe_models = [s for s in model_calibrated.stages if isinstance(s, OneHotEncoderModel)]\n",
    "indexer_models = [s for s in model_calibrated.stages if isinstance(s, StringIndexerModel)]\n",
    "\n",
    "# Build mappings\n",
    "ohe_output_sizes = {}\n",
    "ohe_input_output_map = {}\n",
    "for ohe in ohe_models:\n",
    "    out_col = ohe.getOutputCol()\n",
    "    in_col = ohe.getInputCol()\n",
    "    size = ohe.categorySizes[0]\n",
    "    ohe_output_sizes[out_col] = size\n",
    "    ohe_input_output_map[out_col] = in_col\n",
    "\n",
    "indexer_label_dict = {\n",
    "    indexer.getOutputCol(): indexer.labels for indexer in indexer_models\n",
    "}\n",
    "\n",
    "# Get feature importances\n",
    "rf_model = model_calibrated.stages[-1]\n",
    "importances = rf_model.featureImportances\n",
    "\n",
    "# Collect featureâimportance pairs\n",
    "feature_importance_pairs = []\n",
    "current_index = 0\n",
    "\n",
    "for col in assembler.getInputCols():\n",
    "    if col in numerical_cols:\n",
    "        importance = importances[current_index]\n",
    "        feature_importance_pairs.append((col, importance))\n",
    "        current_index += 1\n",
    "\n",
    "    elif col in ohe_output_sizes:\n",
    "        size = ohe_output_sizes[col]\n",
    "        num_dims = size - 1  # default: dropLast=True\n",
    "\n",
    "        index_col = ohe_input_output_map[col]\n",
    "        labels = indexer_label_dict.get(index_col, [f\"{col}_cat_{i}\" for i in range(num_dims)])\n",
    "        labels = labels[:num_dims]  # due to dropLast=True\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            importance = importances[current_index]\n",
    "            feature_label = f\"{col.replace('_ohe', '')} = {label}\"\n",
    "            feature_importance_pairs.append((feature_label, importance))\n",
    "            current_index += 1\n",
    "    else:\n",
    "        feature_importance_pairs.append((f\"UNKNOWN: {col}\", importances[current_index]))\n",
    "        current_index += 1\n",
    "\n",
    "# Sort by importance\n",
    "sorted_features = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Sorted Random Forest Feature Importances ===\\n\")\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature:40s} -> {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40288719-d438-4b07-9276-3bd8bb6108be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "eda = df_otpw_60_months.orderBy(col(\"FL_DATE\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d9f5ff-9297-413b-be7e-0d5acb6713ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(eda.count(), len(eda.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2a9b04-30ea-421c-b4f6-0e06d76c03c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA of 60 months OTPW Training Dataset\n",
    "\n",
    "# Data Dictionary of Flights Data\n",
    "\n",
    "data_dictionary_otpw_60_months  = {\n",
    "    \"YEAR\": \"Year (int)\",\n",
    "    \"QUARTER\": \"Quarter (1-4) (int)\",\n",
    "    \"MONTH\": \"Month (int)\",\n",
    "    \"DAY_OF_MONTH\": \"Day of Month (int)\",\n",
    "    \"DAY_OF_WEEK\": \"Day of Week (int)\",\n",
    "    \"FL_DATE\": \"Flight Date (yyyymmdd) (String)\",\n",
    "    \"OP_UNIQUE_CARRIER\": \"Unique Carrier Code. When the same code has been used by multiple carriers, a numeric suffix is used for earlier users, for example, PA, PA(1), PA(2). Use this field for  across a range of years. (String)\",\n",
    "    \"OP_CARRIER_FL_NUM\": \"Flight Number (int)\",\n",
    "    \"ORIGIN\": \"Origin Airport (String)\",\n",
    "    \"DEST\": \"Destination Airport (String)\",\n",
    "    \"CRS_DEP_TIME\": \"CRS Departure Time (local time: hhmm) (int)\",\n",
    "    \"DEP_TIME\": \"Actual Departure Time (local time: hhmm) (int)\",\n",
    "    \"DEP_DELAY\": \"Difference in minutes between scheduled and actual departure time. Early departures show negative numbers. (double)\",\n",
    "    \"TAXI_OUT\": \"Taxi Out Time, in Minutes (double)\",\n",
    "    \"WHEELS_OFF\": \"Wheels Off Time (local time: hhmm)) (int)\",\n",
    "    \"WHEELS_ON\": \"Wheels On Time (local time: hhmm) (int)\",\n",
    "    \"TAXI_IN\": \"Taxi In Time, in Minutes (double)\",\n",
    "    \"CRS_ARR_TIME\": \"CRS Arrival Time (local time: hhmm) (int)\",\n",
    "    \"SCHED_DEPART_UNIX\": \"Scheduled Departure Time in Unix epoch (long)\",\n",
    "    \"CANCELLED\": \"Cancelled Flight Indicator (1=Yes) (double)\",\n",
    "    \"DIVERTED\": \"Diverted Flight Indicator (1=Yes) (double)\",\n",
    "    \"CRS_ELAPSED_TIME\": \"CRS Elapsed Time of Flight, in Minutes (double)\",\n",
    "    \"AIR_TIME\": \"Flight Time, in Minutes (double)\",\n",
    "    \"DISTANCE\": \"Distance between airports (miles) (double)\",\n",
    "    \"ORIGIN_TYPE\": \"Origin Airport Type (string)\",\n",
    "    \"ORIGIN_REGION\": \"Origin Airport Region (string)\",\n",
    "    \"ORIGIN_AIRPORT_LAT\": \"Origin Airport Latitude (double)\",\n",
    "    \"ORIGIN_AIRPORT_LON\": \"Origin Airport Longitude (double)\",\n",
    "    \"DEST_TYPE\": \"Destination Airport Type (string)\",\n",
    "    \"DEST_REGION\": \"Destination Airport Region (string)\",\n",
    "    \"DEST_AIRPORT_LAT\": \"Destination Airport Latitude (double)\",\n",
    "    \"DEST_AIRPORT_LON\": \"Destination Airport Longitude (double)\",\n",
    "    \"ELEVATION\": \"Airport Elevation (double)\",\n",
    "    \"HourlyAltimeterSetting\": \"Hourly Altimeter Setting (float)\",\n",
    "    \"HourlyDewPointTemperature\": \"Hourly Dew Point Temperature (float)\",\n",
    "    \"HourlyDryBulbTemperature\": \"Hourly Dry Bulb Temperature (float)\",\n",
    "    \"HourlyPrecipitation\": \"Hourly Precipitation (float)\",\n",
    "    \"HourlyRelativeHumidity\": \"Hourly Relative Humidity (float)\",\n",
    "    \"HourlySkyConditions\": \"Hourly Sky Conditions (string)\",\n",
    "    \"HourlyStationPressure\": \"Hourly Station Pressure (float)\",\n",
    "    \"HourlyVisibility\": \"Hourly Visibility (float)\",\n",
    "    \"HourlyWetBulbTemperature\": \"Hourly Wet Bulb Temperature (float)\",\n",
    "    \"HourlyWindDirection\": \"Hourly Wind Direction (float)\",\n",
    "    \"HourlyWindSpeed\": \"Hourly Wind Speed (float)\",\n",
    "    \"DELAY_TARGET\": \"Delay Target (0=No Delay, 1=Delay) (int)\",\n",
    "    \"HAS_WEATHER_TYPE\": \"Has Weather Type (1=Yes, 0=No) (int)\",\n",
    "    \"IS_HOLIDAY\": \"Is Holiday (1=Yes, 0=No) (string)\",\n",
    "    \"rolling_3mo_delay_count\": \"Rolling 3 Month Delay Count (long)\"\n",
    "}\n",
    "\n",
    "df_dict_otpw_60_months = pd.DataFrame(list(data_dictionary_otpw_60_months.items()), columns=[\"Feature Name\", \"Description\"])\n",
    "display(df_dict_otpw_60_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624c47f6-0e48-4789-8a93-dcf5b5dae7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# train_df EDA continued\n",
    "eda.createOrReplaceTempView(\"otpw_60\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8c9dfd-1a5e-4712-b3fd-4f257fed3cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cancelled Flights\n",
    "spark.sql(\"SELECT count(*) FROM otpw_60 WHERE CANCELLED = 1\").show()\n",
    "# Diverted Flights\n",
    "spark.sql(\"SELECT count(*) FROM otpw_60 WHERE Diverted = 1\").show()\n",
    "# Delayed Flights\n",
    "spark.sql(\"SELECT count(*) FROM otpw_60 WHERE ARR_DELAY > 0\").show()\n",
    "#Total Flights\n",
    "spark.sql(\"SELECT count(*) FROM otpw_60\").show()\n",
    "display(spark.sql(\"SELECT * FROM otpw_60\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce43f08-d796-4368-bb96-3b7621b90360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Causes of Delay\n",
    "display(spark.sql(\"SELECT sum(CARRIER_DELAY), sum(WEATHER_DELAY), sum(NAS_DELAY), sum(SECURITY_DELAY), sum(LATE_AIRCRAFT_DELAY) FROM otpw_60 Where ARR_DELAY > 0 \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e273cc-97a1-4253-aac2-456e77f54002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"SELECT count(*) FROM otpw_60 Where ARR_DELAY > 0 and (CARRIER_DELAY is null and WEATHER_DELAY is null and NAS_DELAY is null and SECURITY_DELAY is null and LATE_AIRCRAFT_DELAY is null  )\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78099653-4424-4e04-b708-9383a5ba397d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"select CAST(ARR_DELAY AS double) as Flight_Delay_In_Minutes from otpw_60 where CANCELLED = 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c6822e-b528-4d68-b20e-c016d213210b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"select count(*) from otpw_60 where CANCELLED = 0 and ARR_DELAY > 0\"))\n",
    "display(spark.sql(\"select count(*) from otpw_60 where CANCELLED = 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad24fbce-32a3-40a9-9afb-4c30c4be8afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"SELECT fl_date, Avg(hourlyvisibility) FROM otpw_60 Where ARR_DELAY > 0 and WEATHER_DELAY is not null group by fl_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3917eb5d-6044-4e56-89a0-1ee141bbc94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"SELECT fl_date, Avg(hourlystationpressure) FROM otpw_60 Where ARR_DELAY > 0 and WEATHER_DELAY is not null group by fl_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574ee64e-3dca-4aff-ba5d-5596477e2417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n",
       "\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5381617944348395>, line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m BinaryClassificationEvaluator(labelCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexedLabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, predictionCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m, metricName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate(predictions)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "\u001B[0;31mTypeError\u001B[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'predictionCol'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"SELECT fl_date, Avg(HourlyPrecipitation) FROM otpw_60 Where ARR_DELAY > 0 and WEATHER_DELAY is not null group by fl_date\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase 3 - Final - Team-1-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}